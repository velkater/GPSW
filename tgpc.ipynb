{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import re\n",
    "import logging\n",
    "import math\n",
    "logging.basicConfig(format='%(message)s', level=logging.ERROR)\n",
    "\n",
    "class Normalizer012:\n",
    "    \"\"\"Object for normalizing ternary directive bi-sequences using the \n",
    "    new normalization algorithm.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialization of the normalization rules checker.\"\"\"\n",
    "        self._rules_checker = _Normalization012_rules_checker()\n",
    "        \n",
    "    def normalize(self, delta, theta):\n",
    "        \"\"\"Ternary normalization algorithm.\n",
    "        \n",
    "        Normalization function that returns the normalized directive\n",
    "        bi-sequence giving the same generalized pseudostandard\n",
    "        word as (delta, theta).\n",
    "\n",
    "        Args:\n",
    "            delta (str): The sequence delta of the directive bi-sequence.\n",
    "                It should be composed of the letters '0', '1' and '2'.\n",
    "            theta (str): The sequence theta of the directive bi-sequence.\n",
    "                It should be composed from the letters 'R', '0', '1' and\n",
    "                '2', where the last three stand for E_0, E_1 and E_2.\n",
    "\n",
    "        Returns:\n",
    "            Returns the tuple `(new_delta, new_theta, notchanged)` where (new_delta, new_theta)\n",
    "            is the normalized bi-sequence of (delta, theta). The boolean `notchanged` is True \n",
    "            if the bi-sequence (delta, theta) was already normalized, otherwise it is False.\n",
    "            \n",
    "        Examples:\n",
    "            >>> n = Normalizer012()\n",
    "            >>> n.normalize(\"0011\", \"00RR\")\n",
    "            ('0011', '00RR', True)\n",
    "            \n",
    "            >>> n.normalize(\"0102110\", \"02R0121\")\n",
    "            ('01021102', '02R01201', False)\n",
    "        \"\"\"\n",
    "        # Checking correct input\n",
    "        _check_dt(delta, theta)\n",
    "        \n",
    "        # Changing the letters to be in order 0,1,2\n",
    "        delta_t, theta_t, substitution = self._change_letters_order(delta, theta)\n",
    "        \n",
    "        # Interleaving delta and theta to get only one sequence from two\n",
    "        biseq = \"\".join(d + t for d, t in zip(delta_t, theta_t))\n",
    "\n",
    "        # Initial pre-processing of the prefix\n",
    "        biseq = self._initial_normalization(biseq)\n",
    "\n",
    "        # The main algorithm:\n",
    "\n",
    "        # Creating a rule checker that find if a normalization rule is applicable\n",
    "        # and returns its correction.\n",
    "        applicable_rule = self._rules_checker.find_applicable_rule(biseq)\n",
    "        \n",
    "        while applicable_rule:\n",
    "            biseq = self._apply_rule(biseq, applicable_rule);\n",
    "            applicable_rule = self._rules_checker.find_applicable_rule(biseq)\n",
    "\n",
    "        # Post-processing\n",
    "        new_delta, new_theta = (biseq[0::2], biseq[1::2])\n",
    "        logging.info(\"bi-sequence before changing the letters back: (\" +\\\n",
    "                         new_delta + \", \" + new_theta + \")\")\n",
    "\n",
    "        new_delta, new_theta = self._change_letters_order_back(new_delta, \n",
    "                                                               new_theta, substitution)\n",
    "        \n",
    "        notchanged = (delta == new_delta) and (theta == new_theta)\n",
    "        return (new_delta, new_theta, notchanged)\n",
    "    \n",
    "    # Preprocessing\n",
    "    @staticmethod\n",
    "    def _substitute(dic, seq):\n",
    "        \"\"\"Substitutes letters in a word according to rules in the dictionary\n",
    "        dic. If there is no rule for the letter, keeps the letter.\n",
    "        \"\"\"\n",
    "        newseq = \"\"\n",
    "        for l in seq:\n",
    "            if l in dic:\n",
    "                newseq = newseq + dic[l]\n",
    "            else:\n",
    "                newseq = newseq + l\n",
    "        return newseq\n",
    "    \n",
    "    @staticmethod\n",
    "    def _compose_substitutions(subs1, subs2):\n",
    "        \"\"\"Composes two substitutions of letters.\"\"\"\n",
    "        csub = {}\n",
    "        for l in [\"0\", \"1\", \"2\"]:\n",
    "            if l in subs1:\n",
    "                csub[l] = subs1[l]\n",
    "                if csub[l] in subs2:\n",
    "                    csub[l] = subs2[csub[l]]\n",
    "            elif l in subs2:\n",
    "                csub[l] = subs2[l]\n",
    "        return csub\n",
    "\n",
    "    def _change_letters_order(self, delta, theta):\n",
    "        \"\"\"Changes (delta, theta) so that the word obtained is the same as the \n",
    "        original one, but the first symbol is 0, the second 1 and the third 2.\n",
    "        \"\"\"\n",
    "        subs = {}\n",
    "        subs2 = {\"2\": \"1\", \"1\": \"2\"}\n",
    "        # changing the first letter to be 0\n",
    "        if delta[0] != \"0\":\n",
    "            subs = {delta[0]: \"0\", \"0\": delta[0]}\n",
    "            delta = self._substitute(subs, delta)\n",
    "            theta = self._substitute(subs, theta)\n",
    "        i = 0\n",
    "        l = len(delta)\n",
    "        \n",
    "        #changing the second letter to 1\n",
    "        while i < l and delta[i] == \"0\":\n",
    "            if theta[i] == \"2\":\n",
    "                return [delta, theta, subs]\n",
    "            if theta[i] == \"1\":\n",
    "                delta = self._substitute(subs2, delta)\n",
    "                theta = self._substitute(subs2, theta)            \n",
    "                return [delta, theta, self._compose_substitutions(subs, subs2)]\n",
    "            i = i + 1\n",
    "            \n",
    "        if i < l and delta[i] == \"2\":\n",
    "            delta = self._substitute(subs2, delta)\n",
    "            theta = self._substitute(subs2, theta) \n",
    "            return [delta, theta, self._compose_substitutions(subs, subs2)]\n",
    "        return [delta, theta, subs]\n",
    "\n",
    "    def _change_letters_order_back(self, delta, theta, subs):\n",
    "        \"\"\"Gives back the original letter order of delta and theta that were \n",
    "        transformed with the substitution subs.\n",
    "        \"\"\"\n",
    "        backsubs = {v:k for k,v in subs.items()}\n",
    "        delta = self._substitute(backsubs, delta)\n",
    "        theta = self._substitute(backsubs, theta)\n",
    "        return [delta, theta]\n",
    "    \n",
    "    @staticmethod\n",
    "    def _initial_normalization(biseq):\n",
    "        \"\"\"Initial preprocessing of the directive bi-sequence so that\n",
    "        the prefix (i^l, {RE_i}^l) is (i^l, E_i^l).\n",
    "        \"\"\"\n",
    "        m = re.match(\"(0(R|0))+\", biseq)\n",
    "        if m:\n",
    "            biseq = \"00\"*int((m.end()-m.start())/2) + biseq[m.end():]\n",
    "        return biseq\n",
    "    \n",
    "    @staticmethod\n",
    "    def _apply_rule(biseq, correction):\n",
    "        \"\"\"Function that applies the correction in the biseq.\"\"\"\n",
    "        return biseq[:correction[0]] + correction[1] + biseq[correction[0] + 2:]\n",
    "    \n",
    "    def print_all_factor_rules(self):\n",
    "        \"\"\"Prints in a readable form all the factor normalization rules\"\"\"\n",
    "        self._rules_checker.print_all_factor_rules_readable\n",
    "        \n",
    "\n",
    "class _Normalization012_rules_checker:\n",
    "    \"\"\"Checks if some normalization rule is applicable and if so,\n",
    "    returns its position and correction.   \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Preparing and compiling all the normalization rules.\n",
    "        \"\"\"\n",
    "        self._ei = {\"0\": Ei(\"0\"), \"1\": Ei(\"1\"), \"2\": Ei(\"2\")}\n",
    "        self._generate_factor_rules()        \n",
    "        self._compile_rules()\n",
    "    \n",
    "    # Regex representing left sides of the prefix normalization rules,\n",
    "    # their corrections (and the identifier of the rule for information).\n",
    "    _bad_prefixes_and_correction = (\n",
    "            (\"(00)*02\", \"0012\", 1),\n",
    "            (\"0010\", \"122100\", 2),\n",
    "            (\"00(120R)+10\", \"1220\", 3),\n",
    "            (\"0012(0R12)*01\", \"0R21\", 4),\n",
    "            (\"001221(1R11)*12\", \"1R22\", 5),\n",
    "            (\"0012211R(111R)*10\", \"1100\", 6),\n",
    "            (\"001222\", \"210012\", 7),\n",
    "            (\"0012(0R12)*00\", \"0R20\", 8),\n",
    "            (\"00(120R)*11\", \"1221\", 9),\n",
    "            (\"(001221)*00122R\", \"211R\", 10),\n",
    "            (\"(001221)*001R\",\"120R\", 11),\n",
    "            (\"(001221)+0R\",\"002R\", 12),\n",
    "            (\"(001221)+1R2R\", \"222R\", 13),\n",
    "            (\"(001221)+002R2R\", \"210R\", 14),\n",
    "            (\"(001221)*00120R2R\", \"201R\", 15),\n",
    "            (\"00(120R)*122111\", \"1R11\", 16),\n",
    "            (\"0012(0R12)*0R2022\", \"2112\", 17),\n",
    "            (\"(00)+1212\", \"1R02\", 18),\n",
    "            (\"0012(0R12)+2020\", \"2R10\", 19),\n",
    "            (\"(00)+1210\", \"1120\", 20),\n",
    "            (\"0012(0R12)*0R2112\", \"1022\", 21),\n",
    "            (\"001221(1R11)*0020\", \"2R10\", 22),\n",
    "            (\"001221(1R11)*1R2201\", \"0021\", 23),\n",
    "            (\"(00)+1202\", \"0R12\", 24),\n",
    "            (\"(00)+001111\", \"1R11\", 25),\n",
    "            (\"(00)+001020\", \"2R10\", 26),\n",
    "            (\"00(120R)+202111\", \"120021\", 27),\n",
    "            (\"(00)+121121\", \"200211\", 28),\n",
    "            (\"00(120R)+211020\", \"220110\", 29),\n",
    "            (\"001221(1R11)*1R220020\", \"211200\", 30)\n",
    "    )\n",
    "            \n",
    "    def find_applicable_rule(self, biseq):\n",
    "        \"\"\"Finds the next applicable normalization rule in the directive bi-sequence.\n",
    "        \n",
    "        Function looking if a prefix rules or a factor rules is applicable inside the\n",
    "        pre-processed directive bi-sequence.\n",
    "\n",
    "        Args:\n",
    "            biseq (str): Directive bi-sequence (interleaved preprocessed sequences delta\n",
    "                and theta).\n",
    "\n",
    "        Returns:\n",
    "            Returns None if no normalization rule is applicable. If there is, it finds\n",
    "            the applicable on the shortest prefix of the directive bi-sequence and returns \n",
    "            the index and the correction to apply.\n",
    "        \"\"\"    \n",
    "        logging.info(\"Checking for an applicable rule in\" + str((biseq[0::2], biseq[1::2])))\n",
    "        \n",
    "        applicable_rule = self._find_next_prefix_rule(biseq)\n",
    "        if applicable_rule:\n",
    "            return applicable_rule\n",
    "        \n",
    "        # If there is no bad prefix, we look for a factor rule\n",
    "        applicable_rule = self._find_next_factor_rule(biseq)\n",
    "        if applicable_rule:\n",
    "            return applicable_rule  \n",
    "    \n",
    "    def _find_next_prefix_rule(self, biseq):\n",
    "        # Looking for a prefix rule\n",
    "        for prefix_rule in self._prefix_rules:\n",
    "            match = re.match(prefix_rule[0], biseq)\n",
    "            if match:\n",
    "                logging.info(\"prefix rule: \" + str(prefix_rule))\n",
    "                index = match.end() - 2\n",
    "                return [index, prefix_rule[1]] # place and correction\n",
    "    \n",
    "    def _find_next_factor_rule(self, biseq):\n",
    "        \"\"\"Find the next applicable factor rule, i.e., the rule that can be \n",
    "        applied on the shortest prefix of the directive bi-sequence.\n",
    "        \"\"\"\n",
    "        matches = []\n",
    "        for rules_index,_rules in self._factor_rules.items():\n",
    "            for rule in _rules:\n",
    "                match = re.match(rule, biseq)\n",
    "                if match:\n",
    "                    logging.info(\"rule\" + str(rules_index) + \": \" + \\\n",
    "                    str(self._print_factor_rule(rule)) +\n",
    "                    \" in biseq \" + str((biseq[0::2], biseq[1::2])))\n",
    "                    \n",
    "                    position = match.end() - 2 # The position to be corrected\n",
    "                    #Index of match and correction:\n",
    "                    matches.append([position, self._factor_rules_replacement(rules_index, match.group(2))])\n",
    "                    biseq = biseq[:position + 3]\n",
    "\n",
    "        logging.debug(\"all non-prefix matches: \" + str(matches))\n",
    "        \n",
    "        # Finding final factor rule (leftmost)\n",
    "        if matches:\n",
    "            final = matches[0]\n",
    "            for rule in matches[1:]:\n",
    "                if rule[0] < final[0]:\n",
    "                    final = rule\n",
    "            logging.debug(\"Final change:\" + str(final))\n",
    "            return final \n",
    "    \n",
    "    def _factor_rules_replacement(self, index, rule):\n",
    "        \"\"\"Finds the correction for a given factor rule, depending\n",
    "        on the type of the rule (1, 2, 3 or 4).\n",
    "        \"\"\"\n",
    "        ei = self._ei\n",
    "        if index == 1:\n",
    "            return rule[4]+\"R\"+ rule[2] + rule[3]\n",
    "        elif index == 2:\n",
    "            return rule[4]+rule[1]+rule[2] + \"R\"\n",
    "        elif index == 3:\n",
    "            return (rule[4]+ei[rule[1]][int(rule[3])] \n",
    "                    + ei[rule[1]][int(ei[rule[3]][int(rule[2])])] + rule[1])\n",
    "        elif index == 4:\n",
    "            return rule[6]+rule[1]+rule[2]+rule[3]+rule[4] + rule[5]\n",
    "        else:\n",
    "            logging.error(\"No correction found.\")\n",
    "    \n",
    "    def _generate_factor_rules(self):\n",
    "        \"\"\"Creates all possible factor rules (of type 1, 2, 3 and 4)\"\"\"\n",
    "        ei = self._ei\n",
    "        a_b = [i[0]+i[1] for i in itertools.product('012', repeat = 2)]\n",
    "        i = [\"0\", \"1\", \"2\"]\n",
    "        \n",
    "        # we consider here b as b_2\n",
    "        self._rules1 = (k[0][0]+ \"R\" + ei[k[1]][int(k[0][1])]+ k[1] + \n",
    "                  k[0][1] + k[1] for k in itertools.product(a_b, i))\n",
    "        self._rules2 = (k[0][0]+ k[1] + ei[k[1]][int(k[0][1])]+ \"R\" + \n",
    "                  k[0][1] +\"R\" for k in itertools.product(a_b, i))\n",
    "\n",
    "        ij = itertools.permutations(\"012\", 2)\n",
    "        # we consider here b as b_1\n",
    "        self._rules3 = (k[0][0] + k[1][0] + k[0][1] + \n",
    "                   k[1][1] + ei[k[1][1]][int(ei[k[1][0]][int(k[0][1])])] + k[1][0] \n",
    "                  for k in itertools.product(a_b, ij))\n",
    "\n",
    "        ijk = itertools.permutations(\"012\", 3)\n",
    "        self._rules4 =(k[0][0] + k[1][0] + k[0][1] + k[1][1] + \n",
    "                 ei[k[1][1]][int(ei[k[1][0]][int(k[0][1])])] + k[1][2] + \n",
    "                    ei[k[1][2]][int(ei[k[1][0]][int(k[0][1])])] + k[1][2]\n",
    "                 for k in itertools.product(a_b, ijk))\n",
    "    def _compile_rules(self):\n",
    "        \"\"\"Compiles de regexes of all the normalization rules.\"\"\"\n",
    "        self._prefix_rules = tuple((re.compile(rule[0]),rule[1],rule[2])\n",
    "                                   for rule in self._bad_prefixes_and_correction)\n",
    "\n",
    "        self._factor_rules = {}\n",
    "        self._factor_rules[1] = tuple( re.compile('^([012R]{2})*('+ rule + ')')\n",
    "                                      for rule in self._rules1 )\n",
    "        self._factor_rules[2] = tuple( re.compile('^([012R]{2})*('+ rule + ')')\n",
    "                                      for rule in self._rules2 )\n",
    "        self._factor_rules[3] = tuple( re.compile('^([012R]{2})*('+ rule + ')')\n",
    "                                      for rule in self._rules3 )\n",
    "        self._factor_rules[4] = tuple( re.compile('^([012R]{2})*('+ rule + ')')\n",
    "                                      for rule in self._rules4 )\n",
    "    \n",
    "    def print_all_factor_rules_readable(self):\n",
    "        \"\"\"Prints all the factor rules\"\"\"\n",
    "        for index in self._factor_rules:\n",
    "            self._print_factor_rules(index)\n",
    "            \n",
    "    def _print_factor_rules(self, index):\n",
    "        \"\"\"Prints factor rules of one type\"\"\"\n",
    "        readable_rules = []\n",
    "        print(\"Factor rules \" + str(index))\n",
    "        for rule in self._factor_rules[index]:\n",
    "            readable_rules.append(self._print_factor_rule(rule))\n",
    "        print(tuple(readable_rules))\n",
    "                                  \n",
    "    def _print_factor_rule(self, rule):\n",
    "        \"\"\"Prints a readable factor rule\"\"\"\n",
    "        rule_from_regex = rule.pattern.split(\"*\")[1][1:-1]\n",
    "        return (rule_from_regex[0::2], rule_from_regex[1::2])\n",
    "\n",
    "class NaiveNormalizer012:\n",
    "    \"\"\"Object for normalizing ternary directive bi-sequences using\n",
    "    a naive algorithm.\n",
    "    \"\"\"\n",
    "        \n",
    "    def normalize(self, delta, theta):\n",
    "        \"\"\"Ternary naive normalization algorithm.\n",
    "        \n",
    "        Naive normalization function that returns the normalized \n",
    "        directive bi-sequence giving the same generalized pseudostandard\n",
    "        word as (delta, theta). It creates the prefixes w_i by palindromic\n",
    "        closures a then checks if those are the only pseudopalindromic\n",
    "        prefixes in the word created by the directive bi-sequence.\n",
    "\n",
    "        Args:\n",
    "            delta (str): The sequence delta of the directive bi-sequence.\n",
    "                It should be composed of the letters '0', '1' and '2'.\n",
    "            theta (str): The sequence theta of the directive bi-sequence.\n",
    "                It should be composed from the letters 'R', '0', '1' and\n",
    "                '2', where the last three stand for E_0, E_1 and E_2. Theta\n",
    "                should be of the same length as delta.\n",
    "\n",
    "        Returns:\n",
    "            Returns the tuple `(new_delta, new_theta, notchanged)` where (new_delta, new_theta)\n",
    "            is the normalized bi-sequence of (delta, theta). The boolean `notchanged` is True \n",
    "            if the bi-sequence (delta, theta) was already normalized, otherwise it is False.\n",
    "            \n",
    "        Examples:\n",
    "            >>> nn = NaiveNormalizer012()\n",
    "            >>> nn.normalize(\"0011\", \"00RR\")\n",
    "            ('0011', '00RR', True)\n",
    "            \n",
    "            >>> nn.normalize(\"0102110\", \"02R0121\")\n",
    "            ('01021102', '02R01201', False)\n",
    "        \"\"\"\n",
    "        # Checking correct input\n",
    "        _check_dt(delta, theta)\n",
    "                         \n",
    "        w = \"\"\n",
    "        l=1\n",
    "        prefixes = []\n",
    "        \n",
    "        # Creating the pseudopalindromic prefixes w_i\n",
    "        for step in range(0,len(delta)):\n",
    "            w = w + delta[step]\n",
    "            if theta[step] == \"R\":\n",
    "                w = make_pal_closure(w)\n",
    "            elif theta[step] in [\"0\", \"1\", \"2\"]:\n",
    "                w = make_eipal_closure(w, theta[step])\n",
    "            else:\n",
    "                logging.error(\"wrong symbol\")\n",
    "                return\n",
    "            prefixes.append(w)\n",
    "        logging.info(\"Prefixes from (delta, theta): \" + str(prefixes))\n",
    "        logging.info(\"Obtained word: \" + w)\n",
    "        \n",
    "        # Finding all the pseudopalindromic prefixes of the word obtained\n",
    "        newdelta = delta[0]\n",
    "        newtheta = \"\"\n",
    "        while l <= len(w):\n",
    "            prefix = w[:l]\n",
    "            res = self._test_palindromicity(prefix)\n",
    "            if res[0] == True:\n",
    "                logging.info(prefix)\n",
    "                if l < len(w):\n",
    "                    newdelta = newdelta + w[l]\n",
    "                newtheta = newtheta + res[1]           \n",
    "            l=l+1\n",
    "            \n",
    "        if newdelta == delta and newtheta == theta:\n",
    "            return (newdelta, newtheta, True)\n",
    "        else:\n",
    "            return (newdelta, newtheta, False)\n",
    "        \n",
    "    @staticmethod\n",
    "    def _test_palindromicity(seq):\n",
    "        \"\"\"Checks if a seq is an palindrome or an Ei-palindrome and \n",
    "        returns its nature. Note that the word ii..i is considered \n",
    "        here as an E_i palindrome.\n",
    "        \"\"\"\n",
    "        if is_eipal(seq,0):\n",
    "            return [True, \"0\"]\n",
    "        elif is_eipal(seq, 1):\n",
    "            return [True, \"1\"]\n",
    "        elif is_eipal(seq, 2):\n",
    "            return [True, \"2\"]\n",
    "        elif is_pal(seq):\n",
    "            return [True, \"R\"]\n",
    "        else:\n",
    "            return [False]\n",
    "\n",
    "def Ei(i):\n",
    "    \"\"\"The involutory antimorphism Ei.\n",
    "\n",
    "    Args:\n",
    "        i : Either 0, 1, 2 or \"0\", \"1\", \"2\"\n",
    "        \n",
    "    Returns:\n",
    "        A tuple corresponding to the involutory antimorphism Ei.\n",
    "\n",
    "    Examples:\n",
    "        >>> Ei(0)\n",
    "        ('0', '2', '1')\n",
    "        >>> Ei(1)\n",
    "        ('2', '1', '0')\n",
    "        >>> Ei(2)\n",
    "        ('1', '0', '2')\n",
    "    \"\"\"\n",
    "    # Checking correct input\n",
    "    if i not in {0, 1, 2, \"0\", \"1\", \"2\"}:\n",
    "        raise ValueError(\"{} is not in A = {{0,1,2}}\".format(i))\n",
    "        \n",
    "    i = int(i)\n",
    "    ei = [0,0,0]\n",
    "    ei[i] = str(i)\n",
    "    ei[(i+1)%3] = str((2+i)%3)\n",
    "    ei[(i+2)%3] = str((1+i)%3)\n",
    "    return tuple(ei)\n",
    "\n",
    "def is_eipal(seq, i):\n",
    "    \"\"\"Checks if a word is an Ei-palindrome.\n",
    "\n",
    "    Args:\n",
    "        seq (string): The word checked composed \n",
    "            of the letters \"0\", \"1\" and \"2\".\n",
    "        i: Pseudopalindromic type, can be either 0, 1, 2, or \n",
    "            \"0\", \"1\", \"2\", standing for E_0, E_1 and E_2.\n",
    "        \n",
    "    Returns:\n",
    "        True if the word is an Ei-palindrome, otherwise False.\n",
    "\n",
    "    Examples:\n",
    "        >>> is_eipal(\"012\", 1)\n",
    "        True\n",
    "        >>> is_eipal(\"002\", 1)\n",
    "        False\n",
    "    \"\"\"\n",
    "    # Checking correct input\n",
    "    if i not in {0, 1, 2, \"0\", \"1\", \"2\"}:\n",
    "        raise ValueError(\"{} is not in A = {{0,1,2}}\".format(i))\n",
    "    _check_ternary(seq)\n",
    "    \n",
    "    ei = Ei(i)\n",
    "    l = len(seq)\n",
    "    if l == 1:\n",
    "        if seq == str(i):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    for x in range(0, math.ceil(l/2)):\n",
    "        if seq[x] != ei[int(seq[l-1-x])]:\n",
    "            return False\n",
    "    return(True)\n",
    "\n",
    "def is_pal(seq):\n",
    "    \"\"\"Checks if a word is an R-palindrome.\n",
    "\n",
    "    Args:\n",
    "        seq (string): The word checked.     \n",
    "\n",
    "    Returns:\n",
    "        True if the word is an Ei-palindrome, otherwise False.\n",
    "\n",
    "    Examples:\n",
    "        >>> is_pal(\"012\")\n",
    "        False\n",
    "        >>> is_pal(\"010\")\n",
    "        True\n",
    "    \"\"\"\n",
    "    l = len(seq)\n",
    "    if l == 1:\n",
    "        return(True)\n",
    "    for x in range(0, l // 2):\n",
    "        if seq[x] != seq[l - 1 - x]:\n",
    "            return(False)\n",
    "    return(True)\n",
    "\n",
    "def make_pal_closure(seq):\n",
    "    \"\"\"Makes palindromic closure of a string.\n",
    "\n",
    "    Args:\n",
    "        seq (string): A word.\n",
    "    Returns:\n",
    "        The palindromic closure of the word.\n",
    "\n",
    "    Examples:\n",
    "        >>> make_pal_closure(\"101\")\n",
    "        '101'\n",
    "        >>> make_pal_closure(\"102\")\n",
    "        '10201'\n",
    "    \"\"\"\n",
    "    if is_pal(seq) == True:\n",
    "        return(seq)\n",
    "    i = 1\n",
    "    while is_pal(seq[i:]) != True:\n",
    "        i = i + 1\n",
    "    logging.debug(\"{0} longest palindromic suffix: {1}\"\n",
    "                  .format(seq, seq[i:]))\n",
    "    closure = seq + seq[i - 1::-1]\n",
    "    return(closure)\n",
    "\n",
    "def make_eipal_closure (seq, i):\n",
    "    \"\"\"Makes Ei-palindromic closure of a string.\n",
    "\n",
    "    Args:\n",
    "        seq (string): A word composed \n",
    "            of the letters \"0\", \"1\" and \"2\".\n",
    "        i: Pseudopalindromic type, can be either 0, 1, 2, or \n",
    "            \"0\", \"1\", \"2\", standing for E_0, E_1 and E_2.\n",
    "    Returns:\n",
    "        The palindromic closure of the word.\n",
    "\n",
    "    Examples:\n",
    "        >>> make_eipal_closure(\"102\", 0)\n",
    "        '102'\n",
    "        >>> make_eipal_closure(\"101\", 1)\n",
    "        '10121'\n",
    "    \"\"\"\n",
    "    # Checking correct input\n",
    "    if i not in {0, 1, 2, \"0\", \"1\", \"2\"}:\n",
    "        raise ValueError(\"{} is not in A = {{0,1,2}}\".format(i))\n",
    "    _check_ternary(seq)\n",
    "    \n",
    "    ei = Ei(i)\n",
    "    if is_eipal(seq, i) == True:\n",
    "        return(seq)\n",
    "    j = 1\n",
    "    while is_eipal(seq[j:], i) != True:\n",
    "        j = j+1\n",
    "    logging.debug(\"{0} longest ei-palindromic suffix : {1}\"\n",
    "                  .format(seq,seq[j:]))\n",
    "    closure = seq\n",
    "    pref = seq[j-1::-1]\n",
    "    for letter in pref:\n",
    "        closure = closure + ei[int(letter)]\n",
    "    return(closure)\n",
    "\n",
    "def make_word012(delta, theta, seed = \"\"):\n",
    "    \"\"\"Makes a ternary GPS word from (delta, theta).\n",
    "\n",
    "    Args:\n",
    "        delta (str): The sequence delta of the directive bi-sequence,\n",
    "            composed of the letters '0', '1' and '2'.\n",
    "        theta (str): The sequence theta of the directive bi-sequence,\n",
    "            composed from the letters 'R', '0', '1' and '2', where \n",
    "            the last three stand for E_0, E_1 and E_2. Must have the \n",
    "            same length as delta.\n",
    "        seed (str): seed (initial w_0), optional.\n",
    "\n",
    "\n",
    "    Returns:\n",
    "        A string made by pseudopalidnromic closure from (delta, theta).\n",
    "\n",
    "    Examples:\n",
    "        >>> make_word012(\"0011\", \"012R\")\n",
    "        '00221112200'\n",
    "    \"\"\"\n",
    "    # Checking correct input\n",
    "    _check_dt(delta, theta)\n",
    "    \n",
    "    # Making w by pseudopalindromic closure\n",
    "    w = seed\n",
    "    for step in range(len(delta)):\n",
    "        w = w + delta[step]\n",
    "        if theta[step] == \"R\":\n",
    "            w = make_pal_closure(w)\n",
    "        elif theta[step] in [\"0\", \"1\", \"2\"]:\n",
    "            w = make_eipal_closure(w, theta[step])\n",
    "        else:\n",
    "            logging.error(\"wrong symbol\")\n",
    "            return\n",
    "        logging.info(\"w{0} = {1}\".format(step+1,w))\n",
    "    return(w)\n",
    "\n",
    "def set_logging(logging_level = \"ERROR\"):\n",
    "    \"\"\"Sets the logging level of the module.\n",
    "    \n",
    "    If the level is set to \"ERROR\", the functions log only errors.\n",
    "    If it is set to \"INFO\" or \"DEBUG\", the fuctions print more \n",
    "    information about how ternary words are being processed.\n",
    "    \n",
    "    Args:\n",
    "        level(str): \"ERROR\" (default), \"INFO\" or \"DEBUG\"\n",
    "    \"\"\"\n",
    "    logging.getLogger().setLevel(logging_level)\n",
    "\n",
    "def _check_ternary(seq):\n",
    "    \"\"\"Raises an error if seq is not in A = {\"0\",\"1\", \"2\"}\"\"\"\n",
    "    if not all([x in set(\"012\") for x in seq]):\n",
    "        raise ValueError(\"{} is not in A = {{0,1,2}}\".format(seq))\n",
    "    \n",
    "def _check_theta(seq):\n",
    "    \"\"\"Raises an error if seq is not in A = {\"0\",\"1\", \"2\", \"R\"}\"\"\"\n",
    "    if not all([x in set(\"012R\") for x in seq]):\n",
    "        raise ValueError(\"{} is not in A = {{0,1,2,R}}\".format(seq))\n",
    "        \n",
    "def _check_dt(delta, theta):\n",
    "    \"\"\"Check if the input delta and theta are correct\"\"\"\n",
    "    _check_ternary(delta)\n",
    "    _check_theta(theta)\n",
    "    if len(delta) != len(theta):\n",
    "        raise ValueError(\"The length of delta and theta are not the same\")\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    import doctest\n",
    "    doctest.testmod()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
