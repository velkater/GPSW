{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import re\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Ei(i):\n",
    "    i = int(i)\n",
    "    ei = [0,0,0]\n",
    "    ei[i] = str(i)\n",
    "    ei[(i+1)%3] = str((2+i)%3)\n",
    "    ei[(i+2)%3] = str((1+i)%3)\n",
    "    return tuple(ei)\n",
    "\n",
    "def make_biseq(delta, theta):\n",
    "    \"\"\"Makes one sequence from the bi-sequence delta and theta.\"\"\"\n",
    "    if len(delta) != len(theta):\n",
    "        print(\"lengths of delta and theta are not equal.\")\n",
    "        return\n",
    "    s = \"\"\n",
    "    for i in range(len(delta)):\n",
    "        s = s + delta[i] + theta[i]\n",
    "    return s\n",
    "\n",
    "def parse_biseq(biseq):\n",
    "    \"\"\"Makes the bi-sequence delta and theta from one sequence.\"\"\"\n",
    "    delta = \"\"\n",
    "    theta = \"\"\n",
    "    for i in range(len(biseq)):\n",
    "        if i % 2 == 0:\n",
    "            delta = delta + biseq[i]\n",
    "        else:\n",
    "            theta = theta + biseq[i]\n",
    "    return [delta, theta]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalization012_rules_checker:\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Prepare rules ...\n",
    "        \"\"\"\n",
    "        self._generate_factor_rules()        \n",
    "        self._compile_rules()\n",
    "    \n",
    "    _bad_prefixes_and_correction = [\n",
    "        [\"(00)*02\", \"0012\", 1], # 1.\n",
    "        [\"0010\", \"122100\", 2], # new added rule 29.\n",
    "        [\"00(120R)+10\", \"1220\", 3], # 2.\n",
    "        [\"0012(0R12)*01\", \"0R21\", 4],  # 3.\n",
    "        [\"001221(1R11)*12\", \"1R22\", 5], # 7.fixed rule and rewrite\n",
    "        [\"0012211R(111R)*10\", \"1100\", 6], # 8.\n",
    "        [\"001222\", \"210012\", 7], # 10.\n",
    "        [\"0012(0R12)*00\", \"0R20\", 8], # 12. fixed error\n",
    "        [\"00(120R)*11\", \"1221\", 9], # 13.-- 9\n",
    "        [\"(001221)*00122R\", \"211R\", 10], # 14. --10\n",
    "        [\"(001221)*001R\",\"120R\", 11], # 15. * because of rule 9 --11\n",
    "        [\"(001221)+0R\",\"002R\", 12], # 16. --12\n",
    "        [\"(001221)+1R2R\", \"222R\", 13], # 17. --13\n",
    "        [\"(001221)+002R2R\", \"210R\", 14], # 19. --14\n",
    "        [\"(001221)*00120R2R\", \"201R\", 15], # 18. --15\n",
    "        [\"00(120R)*122111\", \"1R11\", 16], # 20. -- 16\n",
    "        [\"0012(0R12)*0R2022\", \"2112\", 17], # 21. --17\n",
    "        [\"(00)+1212\", \"1R02\", 18], # 22. --18\n",
    "        [\"0012(0R12)+2020\", \"2R10\", 19], # 23. (can be also + and then rule3)\n",
    "        [\"(00)+1210\", \"1120\", 20], # 24. --20\n",
    "        [\"0012(0R12)*0R2112\", \"1022\", 21], # 25. -- 21\n",
    "        [\"001221(1R11)*0020\", \"2R10\", 22], # rewritten rule 26. -- 22\n",
    "        [\"001221(1R11)*1R2201\", \"0021\", 23], # 27. -- 23\n",
    "        [\"(00)+1202\", \"0R12\", 24], # new added rule 28. !!!! -- 24\n",
    "        [\"(00)+001111\", \"1R11\", 25], # from (011, R11)\n",
    "        [\"(00)+001020\", \"2R10\", 26], # from (012, R00)\n",
    "        [\"00(120R)+202111\", \"120021\", 27], # 1st added rule for 2 pseudopal\n",
    "        [\"(00)+121121\", \"200211\", 28], # 2nd added rule for 2 pseudopal\n",
    "        [\"00(120R)+211020\", \"220110\", 29], # 3rd added rule for 2 pseudopal\n",
    "        [\"001221(1R11)*1R220020\", \"211200\", 30] # 4th rules added for 2 pseudopal\n",
    "        ]\n",
    "    \n",
    "    def print_all_factor_rules_readable(self):\n",
    "        for index in self._factor_rules:\n",
    "            self._print_factor_rules(index)\n",
    "            \n",
    "    def find_applicable_rule(self, biseq):\n",
    "        \"\"\" Function looking is there is a bad prefix of a bad factor\n",
    "        inside the preprocessed biseq. If so, it returns the bad prefix position and the\n",
    "        correction to apply. If not, it returned an emply field (for now)??.\"\"\"\n",
    "    \n",
    "        logging.info(\"Checking for an applicable rule in\" + str(parse_biseq(biseq)))\n",
    "        \n",
    "        applicable_rule = self._find_next_prefix_rule(biseq)\n",
    "        if applicable_rule:\n",
    "            return applicable_rule\n",
    "        \n",
    "        # If there is no bad prefix, we look for a factor rule\n",
    "        applicable_rule = self._find_next_factor_rule(biseq)\n",
    "        if applicable_rule:\n",
    "            return applicable_rule  \n",
    "    \n",
    "    def _find_next_prefix_rule(self, biseq):\n",
    "        # Looking for a prefix rule\n",
    "        for prefix_rule in self._prefix_rules:\n",
    "            match = re.match(prefix_rule[0], biseq)\n",
    "            if match:\n",
    "                logging.debug(\"prefix rule: \" + str(prefix_rule))\n",
    "                index = match.end() - 2\n",
    "                return [index, prefix_rule[1]] # bad prefix to repare\n",
    "    \n",
    "    def _find_next_factor_rule(self, biseq):\n",
    "        matches = []\n",
    "        for rules_index,_rules in self._factor_rules.items():\n",
    "            for rule in _rules:\n",
    "                match = re.match(rule, biseq)\n",
    "                if match:\n",
    "                    logging.debug(\"rule1: \" + \\\n",
    "                    str(parse_biseq(rule.pattern.split(\"*\")[1][1:-1])) + \\\n",
    "                    \" in biseq \" + str(parse_biseq(biseq)))\n",
    "                    \n",
    "                    index = match.end() - 2 # The position to be corrected\n",
    "                    #Index of match and correction:\n",
    "                    matches.append([index, self._factor_rules_replacement(rules_index, match.group(2))])\n",
    "                    biseq = biseq[:index + 3]\n",
    "\n",
    "        logging.info(\"all non-prefix matches: \" + str(matches))\n",
    "        # Final leftmost factor rule\n",
    "        if matches:\n",
    "            final = matches[0]\n",
    "            for rule in matches[1:]:\n",
    "                if rule[0] < final[0]:\n",
    "                    final = rule\n",
    "            logging.debug(\"Final change:\" + str(final))\n",
    "            return final \n",
    "    \n",
    "    @staticmethod\n",
    "    def _factor_rules_replacement(index, rule):\n",
    "        if index == 1:\n",
    "            return rule[4]+\"R\"+ rule[2] + rule[3]\n",
    "        elif index == 2:\n",
    "            return rule[4]+rule[1]+rule[2] + \"R\"\n",
    "        elif index == 3:\n",
    "            return (rule[4]+Ei(rule[1])[int(rule[3])] \n",
    "                    + Ei(rule[1])[int(Ei(rule[3])[int(rule[2])])] + rule[1])\n",
    "        elif index == 4:\n",
    "            return rule[6]+rule[1]+rule[2]+rule[3]+rule[4] + rule[5]\n",
    "        else:\n",
    "            print(\"something went wrong\")\n",
    "    \n",
    "    def _print_factor_rules(self, index):\n",
    "        readable_rules = []\n",
    "        print(\"Factor rules \" + str(index))\n",
    "        for rule in self._factor_rules[index]:\n",
    "            readable_rules.append(parse_biseq(rule.pattern.split(\"*\")[1][1:-1]))\n",
    "        print(readable_rules)\n",
    "    \n",
    "    def _generate_factor_rules(self):\n",
    "        a_b = [i[0]+i[1] for i in itertools.product('012', repeat = 2)]\n",
    "        i = [\"0\", \"1\", \"2\"]\n",
    "        \n",
    "        # we consider here b as b_2\n",
    "        self._rules1 = [ k[0][0]+ \"R\" + Ei(k[1])[int(k[0][1])]+ k[1] + \n",
    "                  k[0][1] +k[1] for k in itertools.product(a_b, i)]\n",
    "        self._rules2 = [ k[0][0]+ k[1] + Ei(k[1])[int(k[0][1])]+ \"R\" + \n",
    "                  k[0][1] +\"R\" for k in itertools.product(a_b, i)]\n",
    "\n",
    "        ij = itertools.permutations(\"012\", 2)\n",
    "        # we consider here b as b_1\n",
    "        self._rules3 = [ k[0][0] + k[1][0] + k[0][1] + \n",
    "                   k[1][1] + Ei(k[1][1])[int(Ei(k[1][0])[int(k[0][1])])] + k[1][0] \n",
    "                  for k in itertools.product(a_b, ij)]\n",
    "\n",
    "        ijk = itertools.permutations(\"012\", 3)\n",
    "        self._rules4 =[ k[0][0] + k[1][0] + k[0][1] + k[1][1] + \n",
    "                 Ei(k[1][1])[int(Ei(k[1][0])[int(k[0][1])])] + k[1][2] + \n",
    "                    Ei(k[1][2])[int(Ei(k[1][0])[int(k[0][1])])] + k[1][2]\n",
    "                 for k in itertools.product(a_b, ijk)]\n",
    "    def _compile_rules(self):\n",
    "        self._prefix_rules = tuple((re.compile(rule[0]),rule[1],rule[2])\n",
    "                                   for rule in self._bad_prefixes_and_correction)\n",
    "\n",
    "        self._factor_rules = {}\n",
    "        self._factor_rules[1] = tuple( re.compile('^([012R]{2})*('+ rule + ')')\n",
    "                                      for rule in self._rules1 )\n",
    "        self._factor_rules[2] = tuple( re.compile('^([012R]{2})*('+ rule + ')')\n",
    "                                      for rule in self._rules2 )\n",
    "        self._factor_rules[3] = tuple( re.compile('^([012R]{2})*('+ rule + ')')\n",
    "                                      for rule in self._rules3 )\n",
    "        self._factor_rules[4] = tuple( re.compile('^([012R]{2})*('+ rule + ')')\n",
    "                                      for rule in self._rules4 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = Normalization012_rules_checker()\n",
    "n.find_applicable_rule(\"haloo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalizer012:\n",
    "    def __init__(self):\n",
    "        self._rules_checker = Normalization012_rules_checker()\n",
    "        \n",
    "    def normalize(self, delta1, theta1):\n",
    "        \"\"\"Returns the normalized directive bi-sequence giving the same GPS word\n",
    "        as (delta1, theta1)\"\"\"\n",
    "        # Changing the letters to be in order 0,1,2\n",
    "        delta, theta, substitution = self._change_letters_order(delta1, theta1)\n",
    "\n",
    "        # Normalization of the prefix\n",
    "        delta, theta = self._initial_normalization(delta, theta)    \n",
    "        biseq = make_biseq(delta, theta)\n",
    "\n",
    "        # The main algorithm:\n",
    "        applicable_rule = self._rules_checker.find_applicable_rule(biseq)\n",
    "        while applicable_rule:\n",
    "            biseq = self._apply_rule(biseq, applicable_rule);\n",
    "            applicable_rule = self._rules_checker.find_applicable_rule(biseq)\n",
    "\n",
    "        # Post-processing\n",
    "        delta, theta = parse_biseq(biseq)\n",
    "        logging.info(\"bi-sequence before changing the letters back: \" +\\\n",
    "                         str(parse_biseq(biseq)))\n",
    "\n",
    "        delta, theta = self._change_letters_order_back(delta, theta, substitution)\n",
    "        notchanged = (delta1 == delta) and (theta1 == theta)\n",
    "        return [notchanged, delta, theta]\n",
    "    \n",
    "    # Preprocessing\n",
    "    @staticmethod\n",
    "    def _substitute(dic, seq):\n",
    "        \"\"\"Substitutes letters in a word according to rules in dic, if there is\n",
    "        no rule for the letter, keeps the letter.\"\"\"\n",
    "        newseq = \"\"\n",
    "        for l in seq:\n",
    "            if l in dic:\n",
    "                newseq = newseq + dic[l]\n",
    "            else:\n",
    "                newseq = newseq + l\n",
    "        return newseq\n",
    "    \n",
    "    @staticmethod\n",
    "    def _compose_substitutions(subs1, subs2):\n",
    "        \"\"\"Composes two substitutions of letter.\"\"\"\n",
    "        csub = {}\n",
    "        for l in [\"0\", \"1\", \"2\"]:\n",
    "            if l in subs1:\n",
    "                csub[l] = subs1[l]\n",
    "                if csub[l] in subs2:\n",
    "                    csub[l] = subs2[csub[l]]\n",
    "            elif l in subs2:\n",
    "                csub[l] = subs2[l]\n",
    "        return csub\n",
    "\n",
    "    def _change_letters_order(self, delta, theta):\n",
    "        \"\"\" Change (delta, theta) so that the word obtained is the same as the \n",
    "        original one, but the first symbol is 0, the second 1 and the third 2.\"\"\"\n",
    "        subs = {}\n",
    "        subs2 = {\"2\": \"1\", \"1\": \"2\"}\n",
    "        if delta[0] != \"0\":\n",
    "            subs = {delta[0]: \"0\", \"0\": delta[0]}\n",
    "            delta = self._substitute(subs, delta)\n",
    "            theta = self._substitute(subs, theta)\n",
    "        i = 0\n",
    "        l = len(delta)\n",
    "        while i < l and delta[i] == \"0\":\n",
    "            if theta[i] == \"2\":\n",
    "                return [delta, theta, subs]\n",
    "            if theta[i] == \"1\":\n",
    "                delta = self._substitute(subs2, delta)\n",
    "                theta = self._substitute(subs2, theta)            \n",
    "                return [delta, theta, compose_substitutions(subs, subs2)]\n",
    "            #otherwise whe have to continue\n",
    "            i = i + 1\n",
    "        if i < l and delta[i] == \"2\":\n",
    "            delta = self._substitute(subs2, delta)\n",
    "            theta = self._substitute(subs2, theta) \n",
    "            return [delta, theta, compose_substitutions(subs, subs2)]\n",
    "        return [delta, theta, subs]\n",
    "\n",
    "    def _change_letters_order_back(self, delta, theta, subs):\n",
    "        \"\"\" Give back the original delta and theta that were transformed with \n",
    "        the substitution subs\"\"\"\n",
    "        backsubs = {v:k for k,v in subs.items()}\n",
    "        delta = self._substitute(backsubs, delta)\n",
    "        theta = self._substitute(backsubs, theta)\n",
    "        return [delta, theta]\n",
    "    \n",
    "    @staticmethod\n",
    "    def _initial_normalization(delta, theta):\n",
    "        biseq = make_biseq(delta, theta)\n",
    "        m = re.match(\"(0(R|0))+\", biseq)\n",
    "        if m:\n",
    "            biseq = \"00\"*int((m.end()-m.start())/2) + biseq[m.end():]\n",
    "        return parse_biseq(biseq)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _apply_rule(biseq, rule):\n",
    "        \"\"\" Function that applies the correction 'rule' in the biseq.\"\"\"\n",
    "        return biseq[:rule[0]] + rule[1] + biseq[rule[0] + 2:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[False, '21202210210', '20R02120120']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta = \"21022101\"\n",
    "theta = \"RR021210\"\n",
    "normalizer = Normalizer012()\n",
    "normalizer.normalize(delta, theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_eipal(seq, i):\n",
    "    \"\"\"Checks if a string seq is an E_i palindrome.\"\"\"\n",
    "    ei = Ei(i)\n",
    "    l = len(seq)\n",
    "    if l == 1:\n",
    "        if seq == str(i):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    for x in range(0, math.ceil(l/2)):\n",
    "        if seq[x] != ei[int(seq[l-1-x])]:\n",
    "            return False\n",
    "    return(True)\n",
    "def is_pal(seq):\n",
    "    \"\"\"Checks if a string is a palindrome.\"\"\"\n",
    "    l = len(seq)\n",
    "    if l == 1:\n",
    "        return(True)\n",
    "    for x in range(0, l // 2):\n",
    "        if seq[x] != seq[l - 1 - x]:\n",
    "            return(False)\n",
    "    return(True)\n",
    "def make_pal_closure(seq):\n",
    "    \"\"\"Makes palindromic closure from a string.\"\"\"\n",
    "    if isPal(seq) == True:\n",
    "        return(seq)\n",
    "    i = 1\n",
    "    while isPal(seq[i:]) != True:\n",
    "        i = i + 1\n",
    "    verboseprint(2,\n",
    "                 \"{0} longest palindromic\\\n",
    "                 closure: {1}\".format(seq, seq[i:]))\n",
    "    closure = seq + seq[i - 1::-1]\n",
    "    return(closure)\n",
    "def makeEipalClosure (seq, i):\n",
    "    \"\"\"Makes E_i-th palindromic closure of a string.\"\"\"\n",
    "    ei = Ei(i)\n",
    "    if isEipal(seq, i) == True:\n",
    "        return(seq)\n",
    "    j = 1\n",
    "    while isEipal(seq[j:], i) != True:\n",
    "        j = j+1\n",
    "    gpc.verboseprint(2, \"    {0} longest palindromic \\\n",
    "                     suffix : {1}\".format(seq,seq[j:]))\n",
    "    closure = seq\n",
    "    pref = seq[j-1::-1]\n",
    "    for letter in pref:\n",
    "        closure = closure + ei[int(letter)]\n",
    "    return(closure)\n",
    "def test_palindromicity(seq):\n",
    "    \"\"\"Checks if a seq is an palindrome or and E-palindrome and \n",
    "    returns its nature.\"\"\"\n",
    "    if isEipal(seq,0):\n",
    "        return [True, \"0\"]\n",
    "    elif isEipal(seq, 1):\n",
    "        return [True, \"1\"]\n",
    "    elif isEipal(seq, 2):\n",
    "        return [True, \"2\"]\n",
    "    elif gpc.isPal(seq):\n",
    "        return [True, \"R\"]\n",
    "    else:\n",
    "        return [False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is012NormalizedNaive(delta, theta, steps):\n",
    "    \"\"\"Checks if delta and theta are normalized and if not, \n",
    "    returns the beginning of the normalized sequence.\"\"\"\n",
    "    w = \"\"\n",
    "    l=1\n",
    "    prefixes = []\n",
    "    for step in range(0,steps):\n",
    "        w = w + delta[step]\n",
    "        if theta[step] == \"R\":\n",
    "            w = gpc.makePalClosure(w)\n",
    "        elif theta[step] in [\"0\", \"1\", \"2\"]:\n",
    "            w = makeEipalClosure(w, theta[step])\n",
    "        else:\n",
    "            print(\"wrong symbol\")\n",
    "            break\n",
    "        prefixes.append(w)\n",
    "    logger.info(\"Prefixes from (delta, theta): \" + str(prefixes))\n",
    "    logger.info(\"Obtained word: \" + w)\n",
    "    newdelta = delta[0]\n",
    "    newtheta = \"\"\n",
    "    while l <= len(w):\n",
    "        prefix = w[:l]\n",
    "        res = test_palindromicity(prefix)\n",
    "        if res[0] == True:\n",
    "            gpc.verboseprint(1, prefix)\n",
    "            if l < len(w):\n",
    "                newdelta = newdelta + w[l]\n",
    "            newtheta = newtheta + res[1]           \n",
    "        l=l+1\n",
    "    if newdelta == delta[:steps] and newtheta == theta[:steps]:\n",
    "        return [True, newdelta, newtheta]\n",
    "    else:\n",
    "        return [False, newdelta, newtheta]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
