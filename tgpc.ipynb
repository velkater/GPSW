{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import re\n",
    "import logging\n",
    "import math\n",
    "logging.basicConfig(format='%(message)s', level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalizer012:\n",
    "    \"\"\"Object for normalizing ternary directive bi-sequences using the \n",
    "    new normalization algorithm.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialization of the normalization rules checker.\"\"\"\n",
    "        self._rules_checker = _Normalization012_rules_checker()\n",
    "        \n",
    "    def normalize(self, delta, theta):\n",
    "        \"\"\"Ternary normalization algorithm.\n",
    "        \n",
    "        Normalization function that returns the normalized directive\n",
    "        bi-sequence giving the same generalized pseudostandard\n",
    "        word as (delta, theta).\n",
    "\n",
    "        Args:\n",
    "            delta (str): The sequence delta of the directive bi-sequence.\n",
    "                It should be composed of the letters '0', '1' and '2'.\n",
    "            theta (str): The sequence theta of the directive bi-sequence.\n",
    "                It should be coomposed from the letters 'R', '0', '1' and\n",
    "                '2', where the last three stand for E_0, E_1 and E_2.\n",
    "\n",
    "        Returns:\n",
    "            Returns the tuple `(new_delta, new_theta, notchanged)` where (new_delta, new_theta)\n",
    "            is the normalized bi-sequence of (delta, theta). The boolean `notchanged` is True \n",
    "            if the bi-sequence (delta, theta) was already normalized, otherwise it is False.\n",
    "            \n",
    "        Examples:\n",
    "            >>> n = Normalizer012()\n",
    "            >>> n.normalize(\"0011\", \"00RR\")\n",
    "            ('0011', '00RR', True)\n",
    "            \n",
    "            >>> n.normalize(\"0102110\", \"02R0121\")\n",
    "            ('01021102', '02R01201', False)\n",
    "        \"\"\"\n",
    "        # Changing the letters to be in order 0,1,2\n",
    "        delta_t, theta_t, substitution = self._change_letters_order(delta, theta)\n",
    "        \n",
    "        # Interleaving delta and theta to get only one sequence from two\n",
    "        biseq = \"\".join(d + t for d, t in zip(delta_t, theta_t))\n",
    "\n",
    "        # Initial pre-processing of the prefix\n",
    "        biseq = self._initial_normalization(biseq)\n",
    "\n",
    "        # The main algorithm:\n",
    "\n",
    "        # Creating a rule checker that find if a normalization rule is applicable\n",
    "        # and returns its correction.\n",
    "        applicable_rule = self._rules_checker.find_applicable_rule(biseq)\n",
    "        \n",
    "        while applicable_rule:\n",
    "            biseq = self._apply_rule(biseq, applicable_rule);\n",
    "            applicable_rule = self._rules_checker.find_applicable_rule(biseq)\n",
    "\n",
    "        # Post-processing\n",
    "        new_delta, new_theta = (biseq[0::2], biseq[1::2])\n",
    "        logging.info(\"bi-sequence before changing the letters back: (\" +\\\n",
    "                         delta + \", \" + theta + \")\")\n",
    "\n",
    "        new_delta, new_theta = self._change_letters_order_back(new_delta, \n",
    "                                                               new_theta, substitution)\n",
    "        \n",
    "        notchanged = (delta == new_delta) and (theta == new_theta)\n",
    "        return (new_delta, new_theta, notchanged)\n",
    "    \n",
    "    # Preprocessing\n",
    "    @staticmethod\n",
    "    def _substitute(dic, seq):\n",
    "        \"\"\"Substitutes letters in a word according to rules in the dictionary\n",
    "        dic. If there is no rule for the letter, keeps the letter.\n",
    "        \"\"\"\n",
    "        newseq = \"\"\n",
    "        for l in seq:\n",
    "            if l in dic:\n",
    "                newseq = newseq + dic[l]\n",
    "            else:\n",
    "                newseq = newseq + l\n",
    "        return newseq\n",
    "    \n",
    "    @staticmethod\n",
    "    def _compose_substitutions(subs1, subs2):\n",
    "        \"\"\"Composes two substitutions of letters.\"\"\"\n",
    "        csub = {}\n",
    "        for l in [\"0\", \"1\", \"2\"]:\n",
    "            if l in subs1:\n",
    "                csub[l] = subs1[l]\n",
    "                if csub[l] in subs2:\n",
    "                    csub[l] = subs2[csub[l]]\n",
    "            elif l in subs2:\n",
    "                csub[l] = subs2[l]\n",
    "        return csub\n",
    "\n",
    "    def _change_letters_order(self, delta, theta):\n",
    "        \"\"\"Changes (delta, theta) so that the word obtained is the same as the \n",
    "        original one, but the first symbol is 0, the second 1 and the third 2.\n",
    "        \"\"\"\n",
    "        subs = {}\n",
    "        subs2 = {\"2\": \"1\", \"1\": \"2\"}\n",
    "        # changing the first letter to be 0\n",
    "        if delta[0] != \"0\":\n",
    "            subs = {delta[0]: \"0\", \"0\": delta[0]}\n",
    "            delta = self._substitute(subs, delta)\n",
    "            theta = self._substitute(subs, theta)\n",
    "        i = 0\n",
    "        l = len(delta)\n",
    "        \n",
    "        #changing the second letter to 1\n",
    "        while i < l and delta[i] == \"0\":\n",
    "            if theta[i] == \"2\":\n",
    "                return [delta, theta, subs]\n",
    "            if theta[i] == \"1\":\n",
    "                delta = self._substitute(subs2, delta)\n",
    "                theta = self._substitute(subs2, theta)            \n",
    "                return [delta, theta, self._compose_substitutions(subs, subs2)]\n",
    "            i = i + 1\n",
    "            \n",
    "        if i < l and delta[i] == \"2\":\n",
    "            delta = self._substitute(subs2, delta)\n",
    "            theta = self._substitute(subs2, theta) \n",
    "            return [delta, theta, self._compose_substitutions(subs, subs2)]\n",
    "        return [delta, theta, subs]\n",
    "\n",
    "    def _change_letters_order_back(self, delta, theta, subs):\n",
    "        \"\"\"Gives back the original letter order of delta and theta that were \n",
    "        transformed with the substitution subs.\n",
    "        \"\"\"\n",
    "        backsubs = {v:k for k,v in subs.items()}\n",
    "        delta = self._substitute(backsubs, delta)\n",
    "        theta = self._substitute(backsubs, theta)\n",
    "        return [delta, theta]\n",
    "    \n",
    "    @staticmethod\n",
    "    def _initial_normalization(biseq):\n",
    "        \"\"\"Initial preprocessing of the directive bi-sequence so that\n",
    "        the prefix (i^l, {RE_i}^l) is (i^l, E_i^l).\n",
    "        \"\"\"\n",
    "        m = re.match(\"(0(R|0))+\", biseq)\n",
    "        if m:\n",
    "            biseq = \"00\"*int((m.end()-m.start())/2) + biseq[m.end():]\n",
    "        return biseq\n",
    "    \n",
    "    @staticmethod\n",
    "    def _apply_rule(biseq, correction):\n",
    "        \"\"\"Function that applies the correction in the biseq.\"\"\"\n",
    "        return biseq[:correction[0]] + correction[1] + correction[rule[0] + 2:]\n",
    "    \n",
    "    def print_all_factor_rules(self):\n",
    "        \"\"\"Prints in a readable form all the factor normalization rules\"\"\"\n",
    "        self._rules_checker.print_all_factor_rules_readable\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class _Normalization012_rules_checker:\n",
    "    \"\"\"Checks if some normalization rule is applicable and if so,\n",
    "    returns its position and correction.   \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Preparing and compiling all the normalization rules.\n",
    "        \"\"\"\n",
    "        self._ei = {\"0\": Ei(\"0\"), \"1\": Ei(\"1\"), \"2\": Ei(\"2\")}\n",
    "        self._generate_factor_rules()        \n",
    "        self._compile_rules()\n",
    "    \n",
    "    # Regex representing left sides of the prefix normalization rules,\n",
    "    # their corrections (and the identifier of the rule for information).\n",
    "    _bad_prefixes_and_correction = (\n",
    "            (\"(00)*02\", \"0012\", 1),\n",
    "            (\"0010\", \"122100\", 2),\n",
    "            (\"00(120R)+10\", \"1220\", 3),\n",
    "            (\"0012(0R12)*01\", \"0R21\", 4),\n",
    "            (\"001221(1R11)*12\", \"1R22\", 5),\n",
    "            (\"0012211R(111R)*10\", \"1100\", 6),\n",
    "            (\"001222\", \"210012\", 7),\n",
    "            (\"0012(0R12)*00\", \"0R20\", 8),\n",
    "            (\"00(120R)*11\", \"1221\", 9),\n",
    "            (\"(001221)*00122R\", \"211R\", 10),\n",
    "            (\"(001221)*001R\",\"120R\", 11),\n",
    "            (\"(001221)+0R\",\"002R\", 12),\n",
    "            (\"(001221)+1R2R\", \"222R\", 13),\n",
    "            (\"(001221)+002R2R\", \"210R\", 14),\n",
    "            (\"(001221)*00120R2R\", \"201R\", 15),\n",
    "            (\"00(120R)*122111\", \"1R11\", 16),\n",
    "            (\"0012(0R12)*0R2022\", \"2112\", 17),\n",
    "            (\"(00)+1212\", \"1R02\", 18),\n",
    "            (\"0012(0R12)+2020\", \"2R10\", 19),\n",
    "            (\"(00)+1210\", \"1120\", 20),\n",
    "            (\"0012(0R12)*0R2112\", \"1022\", 21),\n",
    "            (\"001221(1R11)*0020\", \"2R10\", 22),\n",
    "            (\"001221(1R11)*1R2201\", \"0021\", 23),\n",
    "            (\"(00)+1202\", \"0R12\", 24),\n",
    "            (\"(00)+001111\", \"1R11\", 25),\n",
    "            (\"(00)+001020\", \"2R10\", 26),\n",
    "            (\"00(120R)+202111\", \"120021\", 27),\n",
    "            (\"(00)+121121\", \"200211\", 28),\n",
    "            (\"00(120R)+211020\", \"220110\", 29),\n",
    "            (\"001221(1R11)*1R220020\", \"211200\", 30)\n",
    "    )\n",
    "            \n",
    "    def find_applicable_rule(self, biseq):\n",
    "        \"\"\"Finds the next applicable normalization rule in the directive bi-sequence.\n",
    "        \n",
    "        Function looking if a prefix rules or a factor rules is applicable inside the\n",
    "        pre-processed directive bi-sequence.\n",
    "\n",
    "        Args:\n",
    "            biseq (str): Directive bi-sequence (interleaved preprocessed sequences delta\n",
    "                and theta).\n",
    "\n",
    "        Returns:\n",
    "            Returns None if no normalization rule is applicable. If there is, it finds\n",
    "            the applicable on the shortest prefix of the directive bi-sequence and returns \n",
    "            the index and the correction to apply.\n",
    "        \"\"\"    \n",
    "        logging.info(\"Checking for an applicable rule in\" + str((biseq[0::2], biseq[1::2])))\n",
    "        \n",
    "        applicable_rule = self._find_next_prefix_rule(biseq)\n",
    "        if applicable_rule:\n",
    "            return applicable_rule\n",
    "        \n",
    "        # If there is no bad prefix, we look for a factor rule\n",
    "        applicable_rule = self._find_next_factor_rule(biseq)\n",
    "        if applicable_rule:\n",
    "            return applicable_rule  \n",
    "    \n",
    "    def _find_next_prefix_rule(self, biseq):\n",
    "        # Looking for a prefix rule\n",
    "        for prefix_rule in self._prefix_rules:\n",
    "            match = re.match(prefix_rule[0], biseq)\n",
    "            if match:\n",
    "                logging.debug(\"prefix rule: \" + str(prefix_rule))\n",
    "                index = match.end() - 2\n",
    "                return [index, prefix_rule[1]] # place and correction\n",
    "    \n",
    "    def _find_next_factor_rule(self, biseq):\n",
    "        \"\"\"Find the next applicable factor rule, i.e., the rule that can be \n",
    "        applied on the shortest prefix of the directive bi-sequence.\n",
    "        \"\"\"\n",
    "        matches = []\n",
    "        for rules_index,_rules in self._factor_rules.items():\n",
    "            for rule in _rules:\n",
    "                match = re.match(rule, biseq)\n",
    "                if match:\n",
    "                    logging.debug(\"rule\" + str(rules_index) + \": \" + \\\n",
    "                    str(self._print_factor_rule(rule)) +\n",
    "                    \" in biseq \" + str((biseq[0::2], biseq[1::2])))\n",
    "                    \n",
    "                    position = match.end() - 2 # The position to be corrected\n",
    "                    #Index of match and correction:\n",
    "                    matches.append([position, self._factor_rules_replacement(rules_index, match.group(2))])\n",
    "                    biseq = biseq[:position + 3]\n",
    "\n",
    "        logging.info(\"all non-prefix matches: \" + str(matches))\n",
    "        \n",
    "        # Finding final factor rule (leftmost)\n",
    "        if matches:\n",
    "            final = matches[0]\n",
    "            for rule in matches[1:]:\n",
    "                if rule[0] < final[0]:\n",
    "                    final = rule\n",
    "            logging.debug(\"Final change:\" + str(final))\n",
    "            return final \n",
    "    \n",
    "    def _factor_rules_replacement(self, index, rule):\n",
    "        \"\"\"Finds the correction for a given factor rule, depending\n",
    "        on the type of the rule (1, 2, 3 or 4).\n",
    "        \"\"\"\n",
    "        ei = self._ei\n",
    "        if index == 1:\n",
    "            return rule[4]+\"R\"+ rule[2] + rule[3]\n",
    "        elif index == 2:\n",
    "            return rule[4]+rule[1]+rule[2] + \"R\"\n",
    "        elif index == 3:\n",
    "            return (rule[4]+ei[rule[1]][int(rule[3])] \n",
    "                    + ei[rule[1]][int(ei[rule[3]][int(rule[2])])] + rule[1])\n",
    "        elif index == 4:\n",
    "            return rule[6]+rule[1]+rule[2]+rule[3]+rule[4] + rule[5]\n",
    "        else:\n",
    "            logging.error(\"No correction found.\")\n",
    "    \n",
    "    def _generate_factor_rules(self):\n",
    "        \"\"\"Creates all possible factor rules (of type 1, 2, 3 and 4)\"\"\"\n",
    "        ei = self._ei\n",
    "        a_b = [i[0]+i[1] for i in itertools.product('012', repeat = 2)]\n",
    "        i = [\"0\", \"1\", \"2\"]\n",
    "        \n",
    "        # we consider here b as b_2\n",
    "        self._rules1 = (k[0][0]+ \"R\" + ei[k[1]][int(k[0][1])]+ k[1] + \n",
    "                  k[0][1] + k[1] for k in itertools.product(a_b, i))\n",
    "        self._rules2 = (k[0][0]+ k[1] + ei[k[1]][int(k[0][1])]+ \"R\" + \n",
    "                  k[0][1] +\"R\" for k in itertools.product(a_b, i))\n",
    "\n",
    "        ij = itertools.permutations(\"012\", 2)\n",
    "        # we consider here b as b_1\n",
    "        self._rules3 = (k[0][0] + k[1][0] + k[0][1] + \n",
    "                   k[1][1] + ei[k[1][1]][int(ei[k[1][0]][int(k[0][1])])] + k[1][0] \n",
    "                  for k in itertools.product(a_b, ij))\n",
    "\n",
    "        ijk = itertools.permutations(\"012\", 3)\n",
    "        self._rules4 =(k[0][0] + k[1][0] + k[0][1] + k[1][1] + \n",
    "                 ei[k[1][1]][int(ei[k[1][0]][int(k[0][1])])] + k[1][2] + \n",
    "                    ei[k[1][2]][int(ei[k[1][0]][int(k[0][1])])] + k[1][2]\n",
    "                 for k in itertools.product(a_b, ijk))\n",
    "    def _compile_rules(self):\n",
    "        \"\"\"Compiles de regexes of all the normalization rules.\"\"\"\n",
    "        self._prefix_rules = tuple((re.compile(rule[0]),rule[1],rule[2])\n",
    "                                   for rule in self._bad_prefixes_and_correction)\n",
    "\n",
    "        self._factor_rules = {}\n",
    "        self._factor_rules[1] = tuple( re.compile('^([012R]{2})*('+ rule + ')')\n",
    "                                      for rule in self._rules1 )\n",
    "        self._factor_rules[2] = tuple( re.compile('^([012R]{2})*('+ rule + ')')\n",
    "                                      for rule in self._rules2 )\n",
    "        self._factor_rules[3] = tuple( re.compile('^([012R]{2})*('+ rule + ')')\n",
    "                                      for rule in self._rules3 )\n",
    "        self._factor_rules[4] = tuple( re.compile('^([012R]{2})*('+ rule + ')')\n",
    "                                      for rule in self._rules4 )\n",
    "    \n",
    "    def print_all_factor_rules_readable(self):\n",
    "        \"\"\"Prints all the factor rules\"\"\"\n",
    "        for index in self._factor_rules:\n",
    "            self._print_factor_rules(index)\n",
    "            \n",
    "    def _print_factor_rules(self, index):\n",
    "        \"\"\"Prints factor rules of one type\"\"\"\n",
    "        readable_rules = []\n",
    "        print(\"Factor rules \" + str(index))\n",
    "        for rule in self._factor_rules[index]:\n",
    "            readable_rules.append(self._print_factor_rule(rule))\n",
    "        print(tuple(readable_rules))\n",
    "                                  \n",
    "    def _print_factor_rule(self, rule):\n",
    "        \"\"\"Prints a readable factor rule\"\"\"\n",
    "        rule_from_regex = rule.pattern.split(\"*\")[1][1:-1]\n",
    "        return (rule_from_regex[0::2], rule_from_regex[1::2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NaiveNormalizer012:\n",
    "    \"\"\"Object for normalizing ternary directive bi-sequences using\n",
    "    a naive algorithm.\n",
    "    \"\"\"\n",
    "        \n",
    "    def normalize(self, delta, theta):\n",
    "        \"\"\"Ternary naive normalization algorithm.\n",
    "        \n",
    "        Naive normalization function that returns the normalized \n",
    "        directive bi-sequence giving the same generalized pseudostandard\n",
    "        word as (delta, theta). It creates the prefixes w_i by palindromic\n",
    "        closures a then checks if those are the only pseudopalindromic\n",
    "        prefixes in the word created by the directive bi-sequence.\n",
    "\n",
    "        Args:\n",
    "            delta (str): The sequence delta of the directive bi-sequence.\n",
    "                It should be composed of the letters '0', '1' and '2'.\n",
    "            theta (str): The sequence theta of the directive bi-sequence.\n",
    "                It should be coomposed from the letters 'R', '0', '1' and\n",
    "                '2', where the last three stand for E_0, E_1 and E_2.\n",
    "\n",
    "        Returns:\n",
    "            Returns the tuple `(new_delta, new_theta, notchanged)` where (new_delta, new_theta)\n",
    "            is the normalized bi-sequence of (delta, theta). The boolean `notchanged` is True \n",
    "            if the bi-sequence (delta, theta) was already normalized, otherwise it is False.\n",
    "            \n",
    "        Examples:\n",
    "            >>> nn = NaiveNormalizer012()\n",
    "            >>> nn.normalize(\"0011\", \"00RR\")\n",
    "            ('0011', '00RR', True)\n",
    "            \n",
    "            >>> nn.normalize(\"0102110\", \"02R0121\")\n",
    "            ('01021102', '02R01201', False)\n",
    "        \"\"\"\n",
    "        if len(delta) != len(theta):\n",
    "            logging.error(\"The length of delta and theta must be equal.\")\n",
    "            return\n",
    "                         \n",
    "        w = \"\"\n",
    "        l=1\n",
    "        prefixes = []\n",
    "        \n",
    "        # Creating the pseudopalindromic prefixes w_i\n",
    "        for step in range(0,len(delta)):\n",
    "            w = w + delta[step]\n",
    "            if theta[step] == \"R\":\n",
    "                w = make_pal_closure(w)\n",
    "            elif theta[step] in [\"0\", \"1\", \"2\"]:\n",
    "                w = make_eipal_closure(w, theta[step])\n",
    "            else:\n",
    "                logging.error(\"wrong symbol\")\n",
    "                return\n",
    "            prefixes.append(w)\n",
    "        logging.info(\"Prefixes from (delta, theta): \" + str(prefixes))\n",
    "        logging.info(\"Obtained word: \" + w)\n",
    "        \n",
    "        # Finding all the pseudopalindromic prefixes of the word obtained\n",
    "        newdelta = delta[0]\n",
    "        newtheta = \"\"\n",
    "        while l <= len(w):\n",
    "            prefix = w[:l]\n",
    "            res = self._test_palindromicity(prefix)\n",
    "            if res[0] == True:\n",
    "                logging.info(prefix)\n",
    "                if l < len(w):\n",
    "                    newdelta = newdelta + w[l]\n",
    "                newtheta = newtheta + res[1]           \n",
    "            l=l+1\n",
    "            \n",
    "        if newdelta == delta and newtheta == theta:\n",
    "            return (newdelta, newtheta, True)\n",
    "        else:\n",
    "            return (newdelta, newtheta, False)\n",
    "        \n",
    "    @staticmethod\n",
    "    def _test_palindromicity(seq):\n",
    "        \"\"\"Checks if a seq is an palindrome or an Ei-palindrome and \n",
    "        returns its nature. Note that the word ii..i is considered \n",
    "        here as an E_i palindrome.\n",
    "        \"\"\"\n",
    "        if is_eipal(seq,0):\n",
    "            return [True, \"0\"]\n",
    "        elif is_eipal(seq, 1):\n",
    "            return [True, \"1\"]\n",
    "        elif is_eipal(seq, 2):\n",
    "            return [True, \"2\"]\n",
    "        elif is_pal(seq):\n",
    "            return [True, \"R\"]\n",
    "        else:\n",
    "            return [False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_eipal(seq, i):\n",
    "    \"\"\"Checks if a word is an Ei-palindrome.\n",
    "\n",
    "    Args:\n",
    "        seq (string): The word checked.\n",
    "            It should be composed of the letters '0', '1' and '2'.\n",
    "        i: Pseudopalindromic type, can be either one of the\n",
    "            int 0, 1 or 2 or one of the characters \"0\", \"1\", \"2\".\n",
    "        \n",
    "\n",
    "    Returns:\n",
    "        int: The next number in the range of 0 to `n` - 1.\n",
    "\n",
    "    Examples:\n",
    "        Examples should be written in doctest format, and should illustrate how\n",
    "        to use the function.\n",
    "\n",
    "        >>> print([i for i in example_generator(4)])\n",
    "        [0, 1, 2, 3]\n",
    "\n",
    "    \"\"\"\n",
    "    ei = Ei(i)\n",
    "    l = len(seq)\n",
    "    if l == 1:\n",
    "        if seq == str(i):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    for x in range(0, math.ceil(l/2)):\n",
    "        if seq[x] != ei[int(seq[l-1-x])]:\n",
    "            return False\n",
    "    return(True)\n",
    "def is_pal(seq):\n",
    "    \"\"\"Checks if a string is a palindrome.\"\"\"\n",
    "    l = len(seq)\n",
    "    if l == 1:\n",
    "        return(True)\n",
    "    for x in range(0, l // 2):\n",
    "        if seq[x] != seq[l - 1 - x]:\n",
    "            return(False)\n",
    "    return(True)\n",
    "def make_pal_closure(seq):\n",
    "    \"\"\"Makes palindromic closure from a string.\"\"\"\n",
    "    if is_pal(seq) == True:\n",
    "        return(seq)\n",
    "    i = 1\n",
    "    while is_pal(seq[i:]) != True:\n",
    "        i = i + 1\n",
    "    logging.debug(\"{0} longest palindromic suffix: {1}\"\n",
    "                  .format(seq, seq[i:]))\n",
    "    closure = seq + seq[i - 1::-1]\n",
    "    return(closure)\n",
    "def make_eipal_closure (seq, i):\n",
    "    \"\"\"Makes E_i-th palindromic closure of a string.\"\"\"\n",
    "    ei = Ei(i)\n",
    "    if is_eipal(seq, i) == True:\n",
    "        return(seq)\n",
    "    j = 1\n",
    "    while is_eipal(seq[j:], i) != True:\n",
    "        j = j+1\n",
    "    logging.debug(\"{0} longest ei-palindromic suffix : {1}\"\n",
    "                  .format(seq,seq[j:]))\n",
    "    closure = seq\n",
    "    pref = seq[j-1::-1]\n",
    "    for letter in pref:\n",
    "        closure = closure + ei[int(letter)]\n",
    "    return(closure)\n",
    "def make012Word(delta, theta, steps, seed = \"\"):\n",
    "    \"\"\"Makes a GPS word over {0,1,2} from sequences delta and theta.\"\"\"\n",
    "    w = seed\n",
    "    for step in range(0,steps):\n",
    "        w = w + delta[step]\n",
    "        if theta[step] == \"R\":\n",
    "            w = gpc.makePalClosure(w)\n",
    "        elif theta[step] in [\"0\", \"1\", \"2\"]:\n",
    "            w = makeEipalClosure(w, theta[step])\n",
    "        else:\n",
    "            logging.error(\"wrong symbol\")\n",
    "            return\n",
    "        logging.info(\"w{0} = {1}\".format(step+1,w))\n",
    "    return(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def set_logging(logging_level):\n",
    "    logging.getLogger().setLevel(logging_level)\n",
    "\n",
    "def Ei(i):\n",
    "    i = int(i)\n",
    "    ei = [0,0,0]\n",
    "    ei[i] = str(i)\n",
    "    ei[(i+1)%3] = str((2+i)%3)\n",
    "    ei[(i+2)%3] = str((1+i)%3)\n",
    "    return tuple(ei)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12, '0021']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta = \"0011\"\n",
    "theta = \"00RR\"\n",
    "set_logging(\"ERROR\")\n",
    "normalizer = Normalizer012()\n",
    "\n",
    "biseq = biseq = \"\".join(d + t for d, t in zip(\"0102110\", \"02R0121\"))\n",
    "nc = Normalization012_rules_checker()\n",
    "\n",
    "nc.find_applicable_rule(biseq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Factor rules 1\n",
      "(('000', 'R00'), ('020', 'R11'), ('010', 'R22'), ('021', 'R00'), ('011', 'R11'), ('001', 'R22'), ('012', 'R00'), ('002', 'R11'), ('022', 'R22'), ('100', 'R00'), ('120', 'R11'), ('110', 'R22'), ('121', 'R00'), ('111', 'R11'), ('101', 'R22'), ('112', 'R00'), ('102', 'R11'), ('122', 'R22'), ('200', 'R00'), ('220', 'R11'), ('210', 'R22'), ('221', 'R00'), ('211', 'R11'), ('201', 'R22'), ('212', 'R00'), ('202', 'R11'), ('222', 'R22'))\n",
      "Factor rules 2\n",
      "(('000', '0RR'), ('020', '1RR'), ('010', '2RR'), ('021', '0RR'), ('011', '1RR'), ('001', '2RR'), ('012', '0RR'), ('002', '1RR'), ('022', '2RR'), ('100', '0RR'), ('120', '1RR'), ('110', '2RR'), ('121', '0RR'), ('111', '1RR'), ('101', '2RR'), ('112', '0RR'), ('102', '1RR'), ('122', '2RR'), ('200', '0RR'), ('220', '1RR'), ('210', '2RR'), ('221', '0RR'), ('211', '1RR'), ('201', '2RR'), ('212', '0RR'), ('202', '1RR'), ('222', '2RR'))\n",
      "Factor rules 3\n",
      "(('002', '010'), ('001', '020'), ('001', '101'), ('002', '121'), ('002', '202'), ('001', '212'), ('010', '010'), ('012', '020'), ('012', '101'), ('010', '121'), ('010', '202'), ('012', '212'), ('021', '010'), ('020', '020'), ('020', '101'), ('021', '121'), ('021', '202'), ('020', '212'), ('102', '010'), ('101', '020'), ('101', '101'), ('102', '121'), ('102', '202'), ('101', '212'), ('110', '010'), ('112', '020'), ('112', '101'), ('110', '121'), ('110', '202'), ('112', '212'), ('121', '010'), ('120', '020'), ('120', '101'), ('121', '121'), ('121', '202'), ('120', '212'), ('202', '010'), ('201', '020'), ('201', '101'), ('202', '121'), ('202', '202'), ('201', '212'), ('210', '010'), ('212', '020'), ('212', '101'), ('210', '121'), ('210', '202'), ('212', '212'), ('221', '010'), ('220', '020'), ('220', '101'), ('221', '121'), ('221', '202'), ('220', '212'))\n",
      "Factor rules 4\n",
      "(('0021', '0122'), ('0012', '0211'), ('0012', '1022'), ('0021', '1200'), ('0021', '2011'), ('0012', '2100'), ('0102', '0122'), ('0120', '0211'), ('0120', '1022'), ('0102', '1200'), ('0102', '2011'), ('0120', '2100'), ('0210', '0122'), ('0201', '0211'), ('0201', '1022'), ('0210', '1200'), ('0210', '2011'), ('0201', '2100'), ('1021', '0122'), ('1012', '0211'), ('1012', '1022'), ('1021', '1200'), ('1021', '2011'), ('1012', '2100'), ('1102', '0122'), ('1120', '0211'), ('1120', '1022'), ('1102', '1200'), ('1102', '2011'), ('1120', '2100'), ('1210', '0122'), ('1201', '0211'), ('1201', '1022'), ('1210', '1200'), ('1210', '2011'), ('1201', '2100'), ('2021', '0122'), ('2012', '0211'), ('2012', '1022'), ('2021', '1200'), ('2021', '2011'), ('2012', '2100'), ('2102', '0122'), ('2120', '0211'), ('2120', '1022'), ('2102', '1200'), ('2102', '2011'), ('2120', '2100'), ('2210', '0122'), ('2201', '0211'), ('2201', '1022'), ('2210', '1200'), ('2210', '2011'), ('2201', '2100'))\n"
     ]
    }
   ],
   "source": [
    "nrc = Normalization012_rules_checker()\n",
    "nrc.print_all_factor_rules_readable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
