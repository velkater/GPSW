{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization over the alphabet $\\{0,1,2\\}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gpc # library for work with GPS words over {0,1}\n",
    "import math\n",
    "import itertools\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work with $E_i$-palindromes and closures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Ei(i):\n",
    "    i = int(i)\n",
    "    ei = [0,0,0]\n",
    "    ei[i] = str(i)\n",
    "    ei[(i+1)%3] = str((2+i)%3)\n",
    "    ei[(i+2)%3] = str((1+i)%3)\n",
    "    return ei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2', '1', '0']\n"
     ]
    }
   ],
   "source": [
    "# Example:\n",
    "print(Ei('1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def isEipal(seq, i):\n",
    "    \"\"\"Checks if a string seq is an E_i palindrome.\"\"\"\n",
    "    ei = Ei(i)\n",
    "    l = len(seq)\n",
    "    if l == 1:\n",
    "        if seq == str(i):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    for x in range(0, math.ceil(l/2)):\n",
    "        if seq[x] != ei[int(seq[l-1-x])]:\n",
    "            return False\n",
    "    return(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Example:\n",
    "print(isEipal(\"012\", 1))\n",
    "print(isEipal(\"002\", 1))\n",
    "print(isEipal(\"01201\", 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def testPalindromicity(seq):\n",
    "    \"\"\"Checks if a seq is an palindrome or and E-palindrome and \n",
    "    returns its nature.\"\"\"\n",
    "    if isEipal(seq,0):\n",
    "        return [True, \"0\"]\n",
    "    elif isEipal(seq, 1):\n",
    "        return [True, \"1\"]\n",
    "    elif isEipal(seq, 2):\n",
    "        return [True, \"2\"]\n",
    "    elif gpc.isPal(seq):\n",
    "        return [True, \"R\"]\n",
    "    else:\n",
    "        return [False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[True, '0']\n",
      "[True, '0']\n",
      "[True, 'R']\n",
      "[False]\n"
     ]
    }
   ],
   "source": [
    "# Example:\n",
    "print(testPalindromicity(\"0210\"))\n",
    "print(testPalindromicity(\"00\")) # We want 00 to be an E_0-palindrome\n",
    "print(testPalindromicity(\"010\"))\n",
    "print(testPalindromicity(\"02110\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def makeEipalClosure (seq, i):\n",
    "    \"\"\"Makes E_i-th palindromic closure of a string.\"\"\"\n",
    "    ei = Ei(i)\n",
    "    if isEipal(seq, i) == True:\n",
    "        return(seq)\n",
    "    j = 1\n",
    "    while isEipal(seq[j:], i) != True:\n",
    "        j = j+1\n",
    "    gpc.verboseprint(2, \"    {0} longest palindromic \\\n",
    "                     suffix : {1}\".format(seq,seq[j:]))\n",
    "    closure = seq\n",
    "    pref = seq[j-1::-1]\n",
    "    for letter in pref:\n",
    "        closure = closure + ei[int(letter)]\n",
    "    return(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "012\n",
      "0011\n",
      "00\n",
      "0212102021\n"
     ]
    }
   ],
   "source": [
    "# Example:\n",
    "print(makeEipalClosure(\"01\", 1))\n",
    "print(makeEipalClosure(\"00\", 2))\n",
    "print(makeEipalClosure(\"00\", 0))\n",
    "print(makeEipalClosure(\"021210\", 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make012Word(delta, theta, steps, seed = \"\"):\n",
    "    \"\"\"Makes a GPS over {0,1,2} from sequences delta and theta.\"\"\"\n",
    "    w = seed\n",
    "    for step in range(0,steps):\n",
    "        w = w + delta[step]\n",
    "        if theta[step] == \"R\":\n",
    "            w = gpc.makePalClosure(w)\n",
    "        elif theta[step] in [\"0\", \"1\", \"2\"]:\n",
    "            w = makeEipalClosure(w, theta[step])\n",
    "        else:\n",
    "            print(\"wrong symbol\")\n",
    "            break\n",
    "        gpc.verboseprint(1, \"w{0} = {1}\".format(step+1,w))\n",
    "    return(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'00112200'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example:\n",
    "make012Word(\"0012\", \"0020\", 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive function for checking normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is012NormalizedNaive(delta, theta, steps):\n",
    "    \"\"\"Checks if delta and theta are normalized and if not, \n",
    "    returns the beginning of the normalized sequence.\"\"\"\n",
    "    w = \"\"\n",
    "    l=1\n",
    "    prefixes = []\n",
    "    for step in range(0,steps):\n",
    "        w = w + delta[step]\n",
    "        if theta[step] == \"R\":\n",
    "            w = gpc.makePalClosure(w)\n",
    "        elif theta[step] in [\"0\", \"1\", \"2\"]:\n",
    "            w = makeEipalClosure(w, theta[step])\n",
    "        else:\n",
    "            print(\"wrong symbol\")\n",
    "            break\n",
    "        prefixes.append(w)\n",
    "    gpc.verboseprint(1, \"Prefixes from (delta, theta): \" + str(prefixes))\n",
    "    gpc.verboseprint(1, \"Obtained word: \" + w)\n",
    "    newdelta = delta[0]\n",
    "    newtheta = \"\"\n",
    "    while l <= len(w):\n",
    "        prefix = w[:l]\n",
    "        res = testPalindromicity(prefix)\n",
    "        if res[0] == True:\n",
    "            gpc.verboseprint(1, prefix)\n",
    "            if l < len(w):\n",
    "                newdelta = newdelta + w[l]\n",
    "            newtheta = newtheta + res[1]           \n",
    "        l=l+1\n",
    "    if newdelta == delta[:steps] and newtheta == theta[:steps]:\n",
    "        return [True, newdelta, newtheta]\n",
    "    else:\n",
    "        return [False, newdelta, newtheta]\n",
    "    \n",
    "    # The length of newdelta and newtheta are the same since the whole word \n",
    "    # is an palindrome because if was generated by the GPS construction\n",
    "    # from delta and theta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False, '00120', '00210']\n",
      "[True, '0012', '0021']\n"
     ]
    }
   ],
   "source": [
    "# Example: \n",
    "print(is012NormalizedNaive(\"0012\", \"0020\", 4))\n",
    "print(is012NormalizedNaive(\"00120\", \"00210\", 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of the normalization algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing of the normalized bi-sequence for our implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our implementation of the algorithm, we decided to work only with words\n",
    "that has 0 as first letter, 1 at second and 2 at third in order to make the \n",
    "algorithm easier to read and write. Of course, we want it working for all\n",
    "the bi-sequences, so we have to preprocess the bi-sequence so that the word\n",
    "obtained has first 0, then 1 and then 2. After the result of the algorithm,\n",
    "we will go back to the original letters order. Of course, we want to do it, \n",
    "without having to compute the infinite word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def substitute(dic, seq):\n",
    "    \"\"\"Substitutes letters in a word according to rules in dic, if there is\n",
    "    no rule for the letter, keeps the letter.\"\"\"\n",
    "    newseq = \"\"\n",
    "    for l in seq:\n",
    "        if l in dic:\n",
    "            newseq = newseq + dic[l]\n",
    "        else:\n",
    "            newseq = newseq + l\n",
    "    return newseq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compose(subs1, subs2):\n",
    "    \"\"\"Composes two substitutions of letter.\"\"\"\n",
    "    csub = {}\n",
    "    for l in [\"0\", \"1\", \"2\"]:\n",
    "        if l in subs1:\n",
    "            csub[l] = subs1[l]\n",
    "            if csub[l] in subs2:\n",
    "                csub[l] = subs2[csub[l]]\n",
    "        elif l in subs2:\n",
    "            csub[l] = subs2[l]\n",
    "    return csub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': '1', '1': '0'}\n",
      "{'0': '2', '1': '1', '2': '0'}\n"
     ]
    }
   ],
   "source": [
    "# Example:\n",
    "print(compose({}, {\"1\": \"0\", \"0\": \"1\"}))\n",
    "print(compose({\"0\": \"1\", \"1\": \"0\"}, {\"1\" : \"2\", \"2\" : \"0\", \"0\": \"1\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def changeLettersOrder(delta, theta):\n",
    "    \"\"\" Change (delta, theta) so that the word obtained is the same at the \n",
    "    original one, but the first symbol is 0, the second 1 and the third 2.\"\"\"\n",
    "    subs = {}\n",
    "    subs2 = {\"2\": \"1\", \"1\": \"2\"}\n",
    "    if delta[0] != \"0\":\n",
    "        subs = {delta[0]: \"0\", \"0\": delta[0]}\n",
    "        delta = substitute(subs, delta)\n",
    "        theta = substitute(subs, theta)\n",
    "    i = 0\n",
    "    l = len(delta)\n",
    "    while i < l and delta[i] == \"0\":\n",
    "        if theta[i] == \"2\":\n",
    "            return [delta, theta, subs]\n",
    "        if theta[i] == \"1\":\n",
    "            delta = substitute(subs2, delta)\n",
    "            theta = substitute(subs2, theta)            \n",
    "            return [delta, theta, compose(subs, subs2)]\n",
    "        #otherwise whe have to continue\n",
    "        i = i + 1\n",
    "    if i < l and delta[i] == \"2\":\n",
    "        delta = substitute(subs2, delta)\n",
    "        theta = substitute(subs2, theta) \n",
    "        return [delta, theta, compose(subs, subs2)]\n",
    "    return [delta, theta, subs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def changeLettersOrderBack(delta, theta, subs):\n",
    "    \"\"\" Give back the original delta and theta that were transformed with \n",
    "    the substitution subs\"\"\"\n",
    "    backsubs = {v:k for k,v in subs.items()}\n",
    "    delta = substitute(backsubs, delta)\n",
    "    theta = substitute(backsubs, theta)\n",
    "    return [delta, theta]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['000011112222', '00000RR22001', {'0': '2', '1': '0', '2': '1'}]\n",
      "['111122220000', '11111RR00112'] \n",
      "\n",
      "['000', '002', {'1': '2', '2': '1'}]\n",
      "['000', '001'] \n",
      "\n",
      "['011', '02', {}]\n",
      "['011', '02'] \n",
      "\n",
      "['0001', '0RR0', {'1': '2', '2': '1'}]\n",
      "['0002', '0RR0'] \n",
      "\n",
      "['000012', '0RR02', {'0': '2', '1': '0', '2': '1'}]\n",
      "['111120', '1RR10']\n"
     ]
    }
   ],
   "source": [
    "# Example: \n",
    "ex1 = changeLettersOrder(\"111122220000\", \"11111RR00112\")\n",
    "print(ex1)\n",
    "print(changeLettersOrderBack(ex1[0], ex1[1], ex1[2]), \"\\n\")\n",
    "ex2 = changeLettersOrder(\"000\", \"001\")\n",
    "print(ex2)\n",
    "print(changeLettersOrderBack(ex2[0], ex2[1], ex2[2]), \"\\n\")\n",
    "ex3 = changeLettersOrder(\"011\", \"02\")\n",
    "print(ex3)\n",
    "print(changeLettersOrderBack(ex3[0], ex3[1], ex3[2]), \"\\n\")\n",
    "ex4 = changeLettersOrder(\"0002\", \"0RR0\")\n",
    "print(ex4)\n",
    "print(changeLettersOrderBack(ex4[0], ex4[1], ex4[2]), \"\\n\")\n",
    "ex5 = changeLettersOrder(\"111120\", \"1RR10\")\n",
    "print(ex5)\n",
    "print(changeLettersOrderBack(ex5[0], ex5[1], ex5[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing of the beginning of the bi-sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want that every word beginning with $i^l$ (where $l$ is the largest\n",
    "possible) has a bi-directive sequence $(\\Delta, \\Theta)$ where the prefix\n",
    "of $\\Theta$ of lenght $l$ is equal to $E^l_i$. (We only solve the cases\n",
    "when there is an $R$ instead of $E_0$ (e.g. $(0000, RE_0RE_0) \\to (0000, E_0E_0E_0E_0)$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialNormalization(delta, theta):\n",
    "    biseq = gpc.makeBiseq(delta, theta)\n",
    "    m = re.match(\"(0(R|0))+\", biseq)\n",
    "    if m:\n",
    "        biseq = \"00\"*int((m.end()-m.start())/2) + biseq[m.end():]\n",
    "    return gpc.parseBiseq(biseq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['000011', '000021']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initialNormalization(\"000011\", \"R0R021\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxiliary functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the list of the bad prefixes rewrited for infinite word having first occurences of 0, 1 and 2 in that order.\n",
    "\n",
    "**TO DO** : \n",
    "- check the rewritten rules once more - DONE\n",
    "- get from Pepa the last version - DONE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<img src=\"prefixesrules.jpg\",width=600>\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we rewrite those prefixes in Regex.\n",
    "\n",
    "**TO DO**:\n",
    "- check when $n$ is equal to 0 and when to 1, Pepa said it probably varies\n",
    "- UPDATE: in the 26.th rule, it can be even -1..., check everywhere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# List: bad prefix regex --> the new symbols instead of the last one\n",
    "bad_prefixes = [\n",
    "    [\"(00)*02\", \"0012\", 1], # 1.\n",
    "    [\"0012(0R12)*0R10\", \"1220\", 2], # 2.\n",
    "    [\"0012(0R12)*01\", \"0R21\", 3],  # 3.\n",
    "    [\"00121121\", \"200211\", 4], # 4.\n",
    "    #[\"001210\", \"1120\", 5], # 5. removed, special case of (sco) in 24\n",
    "    #[\"001212\", \"1R02\", 6], # 6. removed, sco 22\n",
    "    [\"001221(1R11)*12\", \"1R22\", 7], # 7.fixed rule and rewrite\n",
    "    [\"0012211R(111R)*10\", \"1100\", 8], # 8.\n",
    "    #[\"001R\", \"120R\", 9], # 9. ! Error, it is not a special case... removed, sco 15\n",
    "    [\"001222\", \"210012\", 10], # 10.\n",
    "    #[\"0011\", \"1221\", 11], # 11. removed sco rule 13 when rewritten\n",
    "    [\"0012(0R12)*00\", \"0R20\", 12], # 12. fixed error\n",
    "    [\"00(120R)*11\", \"1221\", 13], # 13.\n",
    "    [\"(001221)*00122R\", \"211R\", 14], # 14.\n",
    "    [\"(001221)*001R\",\"120R\", 15], # 15. * because of rule 9\n",
    "    [\"(001221)+0R\",\"002R\", 16], # 16.\n",
    "    [\"(001221)+1R2R\", \"222R\", 17], # 17.\n",
    "    [\"(001221)*00120R2R\", \"201R\", 18], # 18.\n",
    "    [\"(001221)+002R2R\", \"210R\", 19], # 19.\n",
    "    [\"00(120R)*122111\", \"1R11\", 20], # 20.\n",
    "    [\"0012(0R12)*0R2022\", \"2112\", 21], # 21.\n",
    "    [\"(00)+1212\", \"1R02\", 22], # 22.\n",
    "    [\"0012(0R12)+2020\", \"2R10\", 23], # 23. (can be also + and then rule3)\n",
    "    [\"(00)*1210\", \"1120\", 24], # 24.\n",
    "    [\"0012(0R12)*0R2112\", \"1022\", 25], # 25.\n",
    "    [\"001221(1R11)*0020\", \"2R10\", 26], # rewritten rule 26.\n",
    "    [\"001221(1R11)*1R2201\", \"0021\", 27], # 27.\n",
    "    [\"001202\", \"0R12\", 28], # new added rule 28.\n",
    "    [\"0010\", \"122100\", 29] # new added rule 29.\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we list all the possible cases of the 4 non-prefix rules. They follow \n",
    "below in a more readable form..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intern rules 1:\n",
      "['0R0000', '0R2101', '0R1202', '0R2010', '0R1111', '0R0212', '0R1020', '0R0121', '0R2222', '1R0000', '1R2101', '1R1202', '1R2010', '1R1111', '1R0212', '1R1020', '1R0121', '1R2222', '2R0000', '2R2101', '2R1202', '2R2010', '2R1111', '2R0212', '2R1020', '2R0121', '2R2222'] \n",
      "\n",
      "Intern rules 2:\n",
      "['000R0R', '012R0R', '021R0R', '002R1R', '011R1R', '020R1R', '001R2R', '010R2R', '022R2R', '100R0R', '112R0R', '121R0R', '102R1R', '111R1R', '120R1R', '101R2R', '110R2R', '122R2R', '200R0R', '212R0R', '221R0R', '202R1R', '211R1R', '220R1R', '201R2R', '210R2R', '222R2R'] \n",
      "\n",
      "Intern rules 3:\n",
      "['000120', '000210', '010011', '010221', '020022', '020112', '001100', '001220', '011021', '011201', '021002', '021122', '002110', '002200', '012001', '012211', '022012', '022102', '100120', '100210', '110011', '110221', '120022', '120112', '101100', '101220', '111021', '111201', '121002', '121122', '102110', '102200', '112001', '112211', '122012', '122102', '200120', '200210', '210011', '210221', '220022', '220112', '201100', '201220', '211021', '211201', '221002', '221122', '202110', '202200', '212001', '212211', '222012', '222102'] \n",
      "\n",
      "Intern rules 4:\n",
      "['00012212', '00021121', '01001222', '01022010', '02002111', '02011020', '00110222', '00122101', '01102202', '01120020', '02100121', '02112000', '00211202', '00220111', '01200212', '01221000', '02201101', '02210010', '10012212', '10021121', '11001222', '11022010', '12002111', '12011020', '10110222', '10122101', '11102202', '11120020', '12100121', '12112000', '10211202', '10220111', '11200212', '11221000', '12201101', '12210010', '20012212', '20021121', '21001222', '21022010', '22002111', '22011020', '20110222', '20122101', '21102202', '21120020', '22100121', '22112000', '20211202', '20220111', '21200212', '21221000', '22201101', '22210010'] \n",
      "\n",
      "-----------\n",
      "More readable rules:\n",
      "Rules 1: (ab_1b_2, RE_iE_i) where b_1=E_i(b_2)\n",
      "[['000', 'R00'], ['020', 'R11'], ['010', 'R22'], ['021', 'R00'], ['011', 'R11'], ['001', 'R22'], ['012', 'R00'], ['002', 'R11'], ['022', 'R22'], ['100', 'R00'], ['120', 'R11'], ['110', 'R22'], ['121', 'R00'], ['111', 'R11'], ['101', 'R22'], ['112', 'R00'], ['102', 'R11'], ['122', 'R22'], ['200', 'R00'], ['220', 'R11'], ['210', 'R22'], ['221', 'R00'], ['211', 'R11'], ['201', 'R22'], ['212', 'R00'], ['202', 'R11'], ['222', 'R22']] \n",
      "\n",
      "Rules 2: (ab_1b_2, E_iRR) where b_1=R(b_2) = b_2\n",
      "[['000', '0RR'], ['020', '1RR'], ['010', '2RR'], ['021', '0RR'], ['011', '1RR'], ['001', '2RR'], ['012', '0RR'], ['002', '1RR'], ['022', '2RR'], ['100', '0RR'], ['120', '1RR'], ['110', '2RR'], ['121', '0RR'], ['111', '1RR'], ['101', '2RR'], ['112', '0RR'], ['102', '1RR'], ['122', '2RR'], ['200', '0RR'], ['220', '1RR'], ['210', '2RR'], ['221', '0RR'], ['211', '1RR'], ['201', '2RR'], ['212', '0RR'], ['202', '1RR'], ['222', '2RR']] \n",
      "\n",
      "Rules 3: (ab_1b_2, E_iE_jE_i) where E_i(b_1)=E_j(b_2)\n",
      "[['002', '010'], ['001', '020'], ['001', '101'], ['002', '121'], ['002', '202'], ['001', '212'], ['010', '010'], ['012', '020'], ['012', '101'], ['010', '121'], ['010', '202'], ['012', '212'], ['021', '010'], ['020', '020'], ['020', '101'], ['021', '121'], ['021', '202'], ['020', '212'], ['102', '010'], ['101', '020'], ['101', '101'], ['102', '121'], ['102', '202'], ['101', '212'], ['110', '010'], ['112', '020'], ['112', '101'], ['110', '121'], ['110', '202'], ['112', '212'], ['121', '010'], ['120', '020'], ['120', '101'], ['121', '121'], ['121', '202'], ['120', '212'], ['202', '010'], ['201', '020'], ['201', '101'], ['202', '121'], ['202', '202'], ['201', '212'], ['210', '010'], ['212', '020'], ['212', '101'], ['210', '121'], ['210', '202'], ['212', '212'], ['221', '010'], ['220', '020'], ['220', '101'], ['221', '121'], ['221', '202'], ['220', '212']] \n",
      "\n",
      "Rules 4: (ab_1b_2b_3, E_iE_jE_kE_k) where E_i(b_1)=E_j(b_2)= E_k(b_3)\n",
      "[['0021', '0122'], ['0012', '0211'], ['0012', '1022'], ['0021', '1200'], ['0021', '2011'], ['0012', '2100'], ['0102', '0122'], ['0120', '0211'], ['0120', '1022'], ['0102', '1200'], ['0102', '2011'], ['0120', '2100'], ['0210', '0122'], ['0201', '0211'], ['0201', '1022'], ['0210', '1200'], ['0210', '2011'], ['0201', '2100'], ['1021', '0122'], ['1012', '0211'], ['1012', '1022'], ['1021', '1200'], ['1021', '2011'], ['1012', '2100'], ['1102', '0122'], ['1120', '0211'], ['1120', '1022'], ['1102', '1200'], ['1102', '2011'], ['1120', '2100'], ['1210', '0122'], ['1201', '0211'], ['1201', '1022'], ['1210', '1200'], ['1210', '2011'], ['1201', '2100'], ['2021', '0122'], ['2012', '0211'], ['2012', '1022'], ['2021', '1200'], ['2021', '2011'], ['2012', '2100'], ['2102', '0122'], ['2120', '0211'], ['2120', '1022'], ['2102', '1200'], ['2102', '2011'], ['2120', '2100'], ['2210', '0122'], ['2201', '0211'], ['2201', '1022'], ['2210', '1200'], ['2210', '2011'], ['2201', '2100']] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "a_b = [i[0]+i[1] for i in itertools.product('012', repeat = 2)]\n",
    "i = [\"0\", \"1\", \"2\"]\n",
    "# we consider here b as b_2\n",
    "rules1 = [ k[0][0]+ \"R\" + Ei(k[1])[int(k[0][1])]+ k[1] + \n",
    "          k[0][1] +k[1] for k in itertools.product(a_b, i)]\n",
    "rules2 = [ k[0][0]+ k[1] + Ei(k[1])[int(k[0][1])]+ \"R\" + \n",
    "          k[0][1] +\"R\" for k in itertools.product(a_b, i)]\n",
    "\n",
    "ij = itertools.permutations(\"012\", 2)\n",
    "#we condider here b as b_1\n",
    "rules3 = [ k[0][0] + k[1][0] + k[0][1] + \n",
    "           k[1][1] + Ei(k[1][1])[int(Ei(k[1][0])[int(k[0][1])])] + k[1][0] \n",
    "          for k in itertools.product(a_b, ij)]\n",
    "\n",
    "ijk = itertools.permutations(\"012\", 3)\n",
    "rules4 =[ k[0][0] + k[1][0] + k[0][1] + k[1][1] + \n",
    "         Ei(k[1][1])[int(Ei(k[1][0])[int(k[0][1])])] + k[1][2] + \n",
    "            Ei(k[1][2])[int(Ei(k[1][0])[int(k[0][1])])] + k[1][2]\n",
    "         for k in itertools.product(a_b, ijk)]\n",
    "\n",
    "\n",
    "print(\"Intern rules 1:\")\n",
    "print(rules1, \"\\n\")\n",
    "print(\"Intern rules 2:\")\n",
    "print(rules2, \"\\n\")\n",
    "print(\"Intern rules 3:\")\n",
    "print(rules3, \"\\n\")\n",
    "print(\"Intern rules 4:\")\n",
    "print(rules4, \"\\n\")\n",
    "\n",
    "# Rules in a more \"readable\" form\n",
    "print(\"-----------\")\n",
    "print(\"More readable rules:\")\n",
    "rules1Readable = [gpc.parseBiseq(rule) for rule in rules1]\n",
    "print( \"Rules 1: (ab_1b_2, RE_iE_i) where b_1=E_i(b_2)\")\n",
    "print(rules1Readable, \"\\n\")\n",
    "\n",
    "rules2Readable = [gpc.parseBiseq(rule) for rule in rules2]\n",
    "print( \"Rules 2: (ab_1b_2, E_iRR) where b_1=R(b_2) = b_2\") # correction\n",
    "print(rules2Readable, \"\\n\") \n",
    "\n",
    "rules3Readable = [gpc.parseBiseq(rule) for rule in rules3]\n",
    "print( \"Rules 3: (ab_1b_2, E_iE_jE_i) where E_i(b_1)=E_j(b_2)\")\n",
    "print(rules3Readable, \"\\n\")  \n",
    "\n",
    "rules4Readable = [gpc.parseBiseq(rule) for rule in rules4]\n",
    "print( \"Rules 4: (ab_1b_2b_3, E_iE_jE_kE_k) where E_i(b_1)=E_j(b_2)= E_k(b_3)\")\n",
    "print(rules4Readable, \"\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def findNextBadFactor(biseq):\n",
    "    \"\"\" Searching for the next (most left) non-prefix rule to apply.\"\"\"\n",
    "    matches = []\n",
    "    for rule in rules1:\n",
    "        match = re.match('([012R][012R])*('+ rule + ')', biseq)\n",
    "        if match:\n",
    "            gpc.verboseprint(1, \"rule1: \" + \\\n",
    "                             str(gpc.parseBiseq(rule)) + \\\n",
    "                             \" in biseq \" + str(gpc.parseBiseq(biseq)))\n",
    "            index = match.end() - 2 # The position that would be corrected\n",
    "            #Here follows the correction:\n",
    "            matches.append([index, rule[4]+\"R\"+ Ei(rule[3])[int(rule[2])] + \n",
    "                           rule[3], 2])\n",
    "            biseq = biseq[:index] # There is no sense searching further\n",
    "    for rule in rules2:\n",
    "        match = re.match('([012R][012R])*('+ rule + ')', biseq)\n",
    "        if match:\n",
    "            gpc.verboseprint(1, \"rule2:  \" + \\\n",
    "                             str(gpc.parseBiseq(rule)) + \\\n",
    "                             \" in biseq \" + str(gpc.parseBiseq(biseq)))\n",
    "            index = match.end() - 2\n",
    "            matches.append([index, rule[4]+rule[1]+rule[2]\n",
    "                            + \"R\", 2])\n",
    "            biseq = biseq[:index]\n",
    "    for rule in rules3:\n",
    "        match = re.match('([012R][012R])*('+ rule + ')', biseq)\n",
    "        if match:\n",
    "            gpc.verboseprint(1, \"rule3: \" + \\\n",
    "                             str(gpc.parseBiseq(rule)) + \\\n",
    "                             \" in biseq \" + str(gpc.parseBiseq(biseq)))\n",
    "            index = match.end() - 2\n",
    "            matches.append([index, rule[4]+Ei(rule[1])[int(rule[3])] \n",
    "                            + Ei(rule[1])[int(Ei(rule[3])[int(rule[2])])] + \n",
    "                            rule[1], 2])\n",
    "            biseq = biseq[:index]\n",
    "    for rule in rules4:\n",
    "        match = re.match('([012R][012R])*('+ rule + ')', biseq)\n",
    "        if match:\n",
    "            gpc.verboseprint(1, \"rule4: \" + \\\n",
    "                             str(gpc.parseBiseq(rule)) + \\\n",
    "                             \" in biseq \" + str(gpc.parseBiseq(biseq)))\n",
    "            index = match.end() - 2\n",
    "            matches.append([index, rule[6]+rule[1]+rule[2]+rule[3]+rule[4] +\n",
    "                            rule[5], 2]) # nema tu byt 4??? asi ne\n",
    "            biseq = biseq[:index]\n",
    "    \n",
    "    gpc.verboseprint(1, \"all non-prefix matches: \" + str(matches))\n",
    "    # Final \"leading\" prefix\n",
    "    final = []\n",
    "    if matches:\n",
    "        final = matches[0]\n",
    "        for rule in matches[1:]:\n",
    "            if rule[0] < final[0]:\n",
    "                final = rule\n",
    "    gpc.verboseprint(1, \"Final change:\" + str(final))\n",
    "    return final  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def isNormalized(biseq):\n",
    "    \"\"\" Function looking is there is a bad prefix of a bad factor\n",
    "    inside the preprocessed biseq. If so, it returns the bad prefix position and the\n",
    "    correction to apply. If not, it returned an emply field (for now).\"\"\"\n",
    "    \n",
    "    gpc.verboseprint(1, \"at the beginning of isNormalized:\" +\\\n",
    "                     str(gpc.parseBiseq(biseq)))\n",
    "    matches = []\n",
    "    # Looking for bad prefixes\n",
    "    for prefixRule in bad_prefixes:\n",
    "        match = re.match(prefixRule[0], biseq)\n",
    "        if match:\n",
    "            gpc.verboseprint(1, \"prefix rule: \" + str(prefixRule))\n",
    "            index = match.end() - 2\n",
    "            return [index, prefixRule[1], 2] # bad prefix to repare\n",
    "            # the third number is the length of the sequence we replace\n",
    "            # so that we know where to continue in the original bi-sequence\n",
    "            \n",
    "    \n",
    "    # If there is no bad prefix, we look for bad factors. We can do it in \n",
    "    # this way because if we have a bad prefix, everything is normalized\n",
    "    # up to its end and therefore there cannot be neither bad factors nor\n",
    "    # other bad prefix before\n",
    "    badfactor = findNextBadFactor(biseq)\n",
    "  \n",
    "    return badfactor # bad factor to repare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def applyRule(biseq, rule):\n",
    "    \"\"\" Function that applies the correction 'rule' in the biseq.\"\"\"\n",
    "    return biseq[:rule[0]] + rule[1] + biseq[rule[0] + rule[2]:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here follows the main normalization algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize012(delta1, theta1):\n",
    "    \"\"\"Returns the normalized directive bi-sequence giving the same GPS word\n",
    "    as (delta, theta)\"\"\"\n",
    "    # Normalization of the letters order\n",
    "    [delta, theta, substitution] = changeLettersOrder(delta1, theta1)\n",
    "    \n",
    "    # Normalization of the prefix\n",
    "    [delta, theta] = initialNormalization(delta, theta)\n",
    "    \n",
    "    # The main algorithm:\n",
    "    biseq = gpc.makeBiseq(delta, theta)\n",
    "    applicableRule = []\n",
    "    applicableRule = isNormalized(biseq)  \n",
    "    \n",
    "    # We do this look until there is no normalization rule to apply\n",
    "    while applicableRule:\n",
    "        biseq = applyRule(biseq, applicableRule);\n",
    "        applicableRule = isNormalized(biseq)\n",
    "    \n",
    "    [delta, theta] = gpc.parseBiseq(biseq)\n",
    "    \n",
    "    gpc.verboseprint(1, \"at the end of isNormalized:\" +\\\n",
    "                     str(gpc.parseBiseq(biseq)))\n",
    "    delta, theta = changeLettersOrderBack(delta, theta, substitution)\n",
    "    notchanged = (delta1 == delta) and (theta1 == theta)\n",
    "    return [notchanged, delta, theta]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We set the verbose for debugging to see the results\n",
    "gpc.verbose = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "Only some \"randomly\" chosen bisequences where tested up to now...\n",
    "\n",
    "The tested bisequences ** that now work well ** can be found in the unit tests in the file test_normalize012.ipynb and we switch directly to the non working cases..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changes:\n",
    "\n",
    "* vytvořilo se nové pravidlo (28): $(010, E_0E_0E_2) \\rightarrow (0101, E_0E_2RE_2)$ (mělo to stejný efekt jako pravidlo $(010 RE_2E_2)$, které v neprefixových pravidlech je, ale v prefixových chybělo)\n",
    "* in the rule 20, we change + to *\n",
    "* we add a new prefix rule (29) from the Pozorovani 3.7 2nd dot for $l=1$ (for $l>1$ it uses rule 28 and then uses other rules). The rule is $(01, 00) -> (0120, 0210)$.\n",
    "* fixing rule 4 that was not normalized: rule 4 $(0112, E_0 E_2 E_1 E_1)$ instead of $(01120, E_0 E_2 E_1 E_0 E_1)$ shoud be rewritten as $(011201, E_0 E_2 E_1 E_0 E_2 E_1)$\n",
    "* the rule 26 was rewritten, because Pepa said $n$ could take the value -1. (the prefix from rules 1 $(012, E_RE_0E_0)$ did not work correctly, the final result was not normalized, it was ['01202', '02100'] instead of [False, '012021', '0210R0']\n",
    "* we checked all the rules for normalization (check once more)\n",
    "* we checked where should be * and where should be +. Now it should be correct.\n",
    "* rule 7 was corrected (2 at the end of the new delta) and rewritten to $(012(11)^n1, E_0E_2E_1(RE_1)^nE_2)$ and was corrected to be $(012(11)^n1, E_0E_2E_1(RE_1)^nE_2) \\rightarrow (012(11)^n12, E_0E_2E_1(RE_1)^nRE_2)$ (the 2 in the end of $\\delta$ was a 0\n",
    "* rule 9 was removed because it is a special case of rule 15\n",
    "* rule 5 was removed because it is a special case of rule 24\n",
    "* rule 6 was removed because it is a special caseof rule 22\n",
    "* rule 13 was rewritten to  (0(10)^n1, E_0(E_2R)^nE_1) a then rule we removed rule 11\n",
    "* the second non-prefix rule was rewritten so that the normalized rule is $(ab_1b_2b_1, E_iRE_iR)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TO DO**\n",
    "- test systematically all the prefixes and find good test cases (maybe ask Pepa for some of them)\n",
    "- fixing the new problematic cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prefixes from (delta, theta): ['0', '010']\n",
      "Obtained word: 010\n",
      "0\n",
      "01\n",
      "010\n",
      "[False, '010', '02R']\n",
      "\n",
      "at the beginning of isNormalized:['01', '0R']\n",
      "all non-prefix matches: []\n",
      "Final change:[]\n",
      "at the end of isNormalized:['01', '0R']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[True, '01', '0R']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpc.verbose = 1\n",
    "delta = \"01\"\n",
    "theta = \"0R\"\n",
    "print(is012NormalizedNaive(delta, theta,len(delta)))\n",
    "print()\n",
    "normalize012(delta, theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The problematic cases..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-prefix rule 2 seems not to work well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prefixes from (delta, theta): ['0', '00', '00100', '00100100']\n",
      "Obtained word: 00100100\n",
      "0\n",
      "00\n",
      "00100\n",
      "00100100\n",
      "[True, '0011', '00RR']\n",
      "\n",
      "at the beginning of isNormalized:['0011', '00RR']\n",
      "all non-prefix matches: []\n",
      "Final change:[]\n",
      "at the end of isNormalized:['0011', '00RR']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[True, '0011', '00RR']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpc.verbose = 1\n",
    "delta = \"0011\"\n",
    "theta = \"00RR\"\n",
    "print(is012NormalizedNaive(delta, theta,len(delta)))\n",
    "print()\n",
    "normalize012(delta, theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prefixes from (delta, theta): ['0', '01', '010', '01010']\n",
      "Obtained word: 01010\n",
      "0\n",
      "01\n",
      "010\n",
      "0101\n",
      "01010\n",
      "[False, '01010', '02R2R']\n",
      "\n",
      "at the beginning of isNormalized:['0101', '02RR']\n",
      "rule2:  ['101', '2RR'] in biseq ['0101', '02RR']\n",
      "all non-prefix matches: [[6, '120R', 2]]\n",
      "Final change:[6, '120R', 2]\n",
      "at the beginning of isNormalized:['01010', '02R2R']\n",
      "all non-prefix matches: []\n",
      "Final change:[]\n",
      "at the end of isNormalized:['01010', '02R2R']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[False, '01010', '02R2R']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpc.verbose = 1\n",
    "delta = \"0101\"\n",
    "theta = \"02RR\"\n",
    "print(is012NormalizedNaive(delta, theta,len(delta)))\n",
    "print()\n",
    "normalize012(delta, theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problems with skipping two pseudopalindromes (cases that are not catched by the algorithm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prefixes from (delta, theta): ['0', '01', '010', '010212', '0102121020', '0102121020210102121020']\n",
      "Obtained word: 0102121020210102121020\n",
      "0\n",
      "01\n",
      "010\n",
      "010212\n",
      "0102121020\n",
      "01021210202101\n",
      "010212102021010212\n",
      "0102121020210102121020\n",
      "[False, '01021201', '02R10210']\n",
      "\n",
      "at the beginning of isNormalized:['010212', '02R100']\n",
      "all non-prefix matches: []\n",
      "Final change:[]\n",
      "at the end of isNormalized:['010212', '02R100']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[True, '010212', '02R100']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpc.verbose = 1\n",
    "delta = \"010212\"\n",
    "theta = \"02R100\"\n",
    "print(is012NormalizedNaive(delta, theta,len(delta)))\n",
    "print()\n",
    "normalize012(delta, theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prefixes from (delta, theta): ['0', '01', '010', '01020', '01020212', '01020212101020212']\n",
      "Obtained word: 01020212101020212\n",
      "0\n",
      "01\n",
      "010\n",
      "01020\n",
      "01020212\n",
      "01020212101\n",
      "01020212101020\n",
      "01020212101020212\n",
      "[False, '01022102', '02R01201']\n",
      "\n",
      "at the beginning of isNormalized:['010221', '02R011']\n",
      "all non-prefix matches: []\n",
      "Final change:[]\n",
      "at the end of isNormalized:['010221', '02R011']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[True, '010221', '02R011']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpc.verbose = 1\n",
    "delta = \"010221\"\n",
    "theta = \"02R011\"\n",
    "print(is012NormalizedNaive(delta, theta,len(delta)))\n",
    "print()\n",
    "normalize012(delta, theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prefixes from (delta, theta): ['0', '00', '0011', '0011122', '0011122200011122']\n",
      "Obtained word: 0011122200011122\n",
      "0\n",
      "00\n",
      "0011\n",
      "0011122\n",
      "0011122200\n",
      "0011122200011\n",
      "0011122200011122\n",
      "[False, '0011201', '0021021']\n",
      "\n",
      "at the beginning of isNormalized:['00112', '00211']\n",
      "all non-prefix matches: []\n",
      "Final change:[]\n",
      "at the end of isNormalized:['00112', '00211']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[True, '00112', '00211']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpc.verbose = 1\n",
    "delta = \"00112\"\n",
    "theta = \"00211\"\n",
    "print(is012NormalizedNaive(delta, theta,len(delta)))\n",
    "print()\n",
    "normalize012(delta, theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Testing interface - this is how we caught the problem with skipping two pseudopalindromes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gpc.verbose = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['0120', '2100'], ['0021', '2011'], ['0102', '0122'], ['0012', '0211'], ['0210', '2011'], ['0012', '2100'], ['0201', '2100'], ['0120', '0211'], ['0102', '2011']]\n",
      "\n",
      "0021 2011\n",
      "Prefixes from (delta, theta): ['01', '01020', '01020212', '01020212101020212']\n",
      "Obtained word: 01020212101020212\n",
      "0\n",
      "01\n",
      "010\n",
      "01020\n",
      "01020212\n",
      "01020212101\n",
      "01020212101020\n",
      "01020212101020212\n",
      "[False, '01022102', '02R01201']\n",
      "\n",
      "at the beginning of isNormalized:['0021', '2011']\n",
      "prefix rule: ['(00)*02', '0012', 1]\n",
      "at the beginning of isNormalized:['01021', '02011']\n",
      "prefix rule: ['0012(0R12)*00', '0R20', 12]\n",
      "at the beginning of isNormalized:['010221', '02R011']\n",
      "all non-prefix matches: []\n",
      "Final change:[]\n",
      "at the end of isNormalized:['010221', '02R011']\n",
      "[False, '010221', '02R011']\n",
      "---------\n",
      "0012 0211\n",
      "Prefixes from (delta, theta): ['0', '0011', '0011122', '0011122200011122']\n",
      "Obtained word: 0011122200011122\n",
      "0\n",
      "00\n",
      "0011\n",
      "0011122\n",
      "0011122200\n",
      "0011122200011\n",
      "0011122200011122\n",
      "[False, '0011201', '0021021']\n",
      "\n",
      "at the beginning of isNormalized:['0012', '0211']\n",
      "prefix rule: ['(00)*02', '0012', 1]\n",
      "at the beginning of isNormalized:['00112', '00211']\n",
      "all non-prefix matches: []\n",
      "Final change:[]\n",
      "at the end of isNormalized:['00112', '00211']\n",
      "[False, '00112', '00211']\n",
      "---------\n",
      "0012 2100\n",
      "Prefixes from (delta, theta): ['01', '010212', '0102121020', '0102121020210102121020']\n",
      "Obtained word: 0102121020210102121020\n",
      "0\n",
      "01\n",
      "010\n",
      "010212\n",
      "0102121020\n",
      "01021210202101\n",
      "010212102021010212\n",
      "0102121020210102121020\n",
      "[False, '01021201', '02R10210']\n",
      "\n",
      "at the beginning of isNormalized:['0012', '2100']\n",
      "prefix rule: ['(00)*02', '0012', 1]\n",
      "at the beginning of isNormalized:['01012', '02100']\n",
      "prefix rule: ['0012(0R12)*01', '0R21', 3]\n",
      "at the beginning of isNormalized:['010212', '02R100']\n",
      "all non-prefix matches: []\n",
      "Final change:[]\n",
      "at the end of isNormalized:['010212', '02R100']\n",
      "[False, '010212', '02R100']\n",
      "---------\n"
     ]
    }
   ],
   "source": [
    "# Testing rules, when they are prefixes, if the normalization is correct\n",
    "setofrules = set()\n",
    "gpc.verbose = 0\n",
    "for rule in rules4:\n",
    "    d = gpc.parseBiseq(rule)[0]\n",
    "    t = gpc.parseBiseq(rule)[1]\n",
    "    change =  changeLettersOrder(d,t)\n",
    "    setofrules.add(gpc.makeBiseq(change[0],change[1]))\n",
    "\n",
    "print(list(map(gpc.parseBiseq, setofrules)))\n",
    "print()\n",
    "    \n",
    "for rule in setofrules:\n",
    "    d = gpc.parseBiseq(rule)[0]\n",
    "    t = gpc.parseBiseq(rule)[1]\n",
    "    rnaive = is012NormalizedNaive(d,t, len(t))\n",
    "    r = normalize012(d,t)\n",
    "    if rnaive != r:\n",
    "        gpc.verbose  = 1\n",
    "        print(d,t)\n",
    "        print(is012NormalizedNaive(d,t, len(t)))\n",
    "        print()\n",
    "        print(normalize012(d,t))\n",
    "        print('---------')\n",
    "        gpc.verbose = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gpc.verbose = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
